{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\arik_\\anaconda3\\envs\\llm_sem\\Lib\\site-packages\\umap\\distances.py:1063: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "c:\\Users\\arik_\\anaconda3\\envs\\llm_sem\\Lib\\site-packages\\umap\\distances.py:1071: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "c:\\Users\\arik_\\anaconda3\\envs\\llm_sem\\Lib\\site-packages\\umap\\distances.py:1086: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "c:\\Users\\arik_\\anaconda3\\envs\\llm_sem\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\arik_\\anaconda3\\envs\\llm_sem\\Lib\\site-packages\\umap\\umap_.py:660: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "c:\\Users\\arik_\\anaconda3\\envs\\llm_sem\\Lib\\site-packages\\umap\\plot.py:203: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\arik_\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\arik_\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pickle \n",
    "from TopicGPT import TopicGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "api_key_openai = os.environ.get('OPENAI_API_KEY2')\n",
    "#org_openai = os.environ.get('OPENAI_ORG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../Data/data_raw_20ng.pkl\", \"rb\") as file:\n",
    "    data = pickle.load(file)\n",
    "\n",
    "# identify and remove empty documents\n",
    "empty_doc_idx = [i for i, document in enumerate(data) if len(document) == 0]\n",
    "data = [document for i, document in enumerate(data) if i not in empty_doc_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm = TopicGPT(\n",
    "    openai_api_key = api_key_openai,\n",
    "    n_topics = 20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18466/18466 [00:12<00:00, 1536.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 6037647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num_tokens = tm.embedder.compute_number_of_tokens(data)\n",
    "print(f\"Number of tokens: {num_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18466/18466 [00:15<00:00, 1203.64it/s]\n",
      "100%|██████████| 18466/18466 [2:12:15<00:00,  2.33it/s]   \n",
      "Processing corpus: 100%|██████████| 18466/18466 [02:02<00:00, 150.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common words:\n",
      "n't: 16205\n",
      "would: 10820\n",
      "one: 10127\n",
      "people: 6428\n",
      "like: 6409\n",
      "get: 5815\n",
      "know: 5742\n",
      "also: 5588\n",
      "use: 4987\n",
      "think: 4982\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20817/20817 [00:00<00:00, 32278.79it/s]\n",
      "100%|██████████| 20817/20817 [2:02:17<00:00,  2.84it/s]  \n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "float() argument must be a string or a real number, not 'dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m tm\u001b[39m.\u001b[39;49mfit(data)\n",
      "File \u001b[1;32mc:\\Users\\arik_\\Documents\\Dokumente\\Studium\\Master\\LLM_seminar\\Project\\src\\TopicGPT\\TopicGPT.py:177\u001b[0m, in \u001b[0;36mTopicGPT.fit\u001b[1;34m(self, corpus)\u001b[0m\n\u001b[0;32m    175\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvocab_embeddings \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdocument_embeddings \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    176\u001b[0m     \u001b[39mif\u001b[39;00m verbose:\n\u001b[1;32m--> 177\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mComputing embeddings...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    178\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_embeddings(corpus \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcorpus)\n\u001b[0;32m    180\u001b[0m \u001b[39mif\u001b[39;00m verbose: \n",
      "File \u001b[1;32mc:\\Users\\arik_\\Documents\\Dokumente\\Studium\\Master\\LLM_seminar\\Project\\src\\TopicGPT\\TopicGPT.py:134\u001b[0m, in \u001b[0;36mTopicGPT.extract_topics\u001b[1;34m(self, corpus)\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvocab \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m: \n\u001b[0;32m    132\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvocab \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mextractor\u001b[39m.\u001b[39mcompute_corpus_vocab(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcorpus, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_vocab_hyperparams)\n\u001b[1;32m--> 134\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtopic_lis \u001b[39m=\u001b[39m TopicRepresentation\u001b[39m.\u001b[39;49mextract_topics_no_new_vocab_computation(\n\u001b[0;32m    135\u001b[0m     corpus \u001b[39m=\u001b[39;49m corpus,\n\u001b[0;32m    136\u001b[0m     vocab \u001b[39m=\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvocab,\n\u001b[0;32m    137\u001b[0m     document_embeddings \u001b[39m=\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdocument_embeddings,\n\u001b[0;32m    138\u001b[0m     clusterer \u001b[39m=\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclusterer,\n\u001b[0;32m    139\u001b[0m     vocab_embeddings \u001b[39m=\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvocab_embeddings,\n\u001b[0;32m    140\u001b[0m     n_topwords \u001b[39m=\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_topwords,\n\u001b[0;32m    141\u001b[0m     topword_extraction_methods \u001b[39m=\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtopword_extraction_methods,\n\u001b[0;32m    142\u001b[0m )\n\u001b[0;32m    144\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtopic_lis\n",
      "File \u001b[1;32mc:\\Users\\arik_\\Documents\\Dokumente\\Studium\\Master\\LLM_seminar\\Project\\src\\TopicRepresentation\\TopicRepresentation.py:255\u001b[0m, in \u001b[0;36mextract_topics_no_new_vocab_computation\u001b[1;34m(corpus, vocab, document_embeddings, clusterer, vocab_embeddings, n_topwords, topword_extraction_methods)\u001b[0m\n\u001b[0;32m    252\u001b[0m \u001b[39mif\u001b[39;00m topword_extraction_methods \u001b[39m==\u001b[39m []:\n\u001b[0;32m    253\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mtopword_extraction_methods cannot be empty\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 255\u001b[0m dim_red_embeddings, labels, umap_mapper \u001b[39m=\u001b[39m clusterer\u001b[39m.\u001b[39;49mcluster_and_reduce(document_embeddings)  \u001b[39m# get dimensionality reduced embeddings, their labels and the umap mapper object\u001b[39;00m\n\u001b[0;32m    257\u001b[0m unique_labels \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39munique(labels)  \u001b[39m# In case the cluster labels are not consecutive numbers, we need to map them to consecutive \u001b[39;00m\n\u001b[0;32m    258\u001b[0m label_mapping \u001b[39m=\u001b[39m {label: i \u001b[39mfor\u001b[39;00m i, label \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(unique_labels[unique_labels \u001b[39m!=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])}\n",
      "File \u001b[1;32mc:\\Users\\arik_\\Documents\\Dokumente\\Studium\\Master\\LLM_seminar\\Project\\src\\Clustering\\Clustering.py:118\u001b[0m, in \u001b[0;36mClustering_and_DimRed.cluster_and_reduce\u001b[1;34m(self, embeddings)\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcluster_and_reduce\u001b[39m(\u001b[39mself\u001b[39m, embeddings: np\u001b[39m.\u001b[39mndarray) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mtuple\u001b[39m[np\u001b[39m.\u001b[39mndarray, np\u001b[39m.\u001b[39mndarray, umap\u001b[39m.\u001b[39mUMAP]:\n\u001b[0;32m    109\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    110\u001b[0m \u001b[39m    Cluster embeddings with HDBSCAN and reduce dimensions with UMAP.\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \u001b[39m    params:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[39m        umap.UMAP, UMAP mapper to transform new embeddings, especially embeddings of the vocabulary (MAKE SURE TO NORMALIZE EMBEDDINGS AFTER USING THE MAPPER)\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 118\u001b[0m     dim_red_embeddings, umap_mapper \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduce_dimensions_umap(embeddings)\n\u001b[0;32m    119\u001b[0m     clusters \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcluster_hdbscan(dim_red_embeddings)\n\u001b[0;32m    120\u001b[0m     \u001b[39mreturn\u001b[39;00m dim_red_embeddings, clusters, umap_mapper\n",
      "File \u001b[1;32mc:\\Users\\arik_\\Documents\\Dokumente\\Studium\\Master\\LLM_seminar\\Project\\src\\Clustering\\Clustering.py:77\u001b[0m, in \u001b[0;36mClustering_and_DimRed.reduce_dimensions_umap\u001b[1;34m(self, embeddings)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mreduce_dimensions_umap\u001b[39m(\u001b[39mself\u001b[39m, embeddings: np\u001b[39m.\u001b[39mndarray) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m np\u001b[39m.\u001b[39mndarray:\n\u001b[0;32m     69\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     70\u001b[0m \u001b[39m    Reduce dimensions with UMAP.\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39m    params:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[39m        umap.UMAP, UMAP mapper to transform new embeddings, especially embeddings of the vocabulary (MAKE SURE TO NORMALIZE EMBEDDINGS AFTER USING THE MAPPER)\u001b[39;00m\n\u001b[0;32m     76\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 77\u001b[0m     mapper \u001b[39m=\u001b[39m umap\u001b[39m.\u001b[39;49mUMAP(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mUMAP_hyperparams)\u001b[39m.\u001b[39;49mfit(embeddings)\n\u001b[0;32m     78\u001b[0m     dim_red_embeddings \u001b[39m=\u001b[39m mapper\u001b[39m.\u001b[39mtransform(embeddings)\n\u001b[0;32m     79\u001b[0m     dim_red_embeddings \u001b[39m=\u001b[39m dim_red_embeddings\u001b[39m/\u001b[39mnp\u001b[39m.\u001b[39mlinalg\u001b[39m.\u001b[39mnorm(dim_red_embeddings, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\arik_\\anaconda3\\envs\\llm_sem\\Lib\\site-packages\\umap\\umap_.py:2269\u001b[0m, in \u001b[0;36mUMAP.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m   2249\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, X, y\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m   2250\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Fit X into an embedded space.\u001b[39;00m\n\u001b[0;32m   2251\u001b[0m \n\u001b[0;32m   2252\u001b[0m \u001b[39m    Optionally use y for supervised dimension reduction.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2266\u001b[0m \u001b[39m        ``target_metric_kwds``.\u001b[39;00m\n\u001b[0;32m   2267\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2269\u001b[0m     X \u001b[39m=\u001b[39m check_array(X, dtype\u001b[39m=\u001b[39;49mnp\u001b[39m.\u001b[39;49mfloat32, accept_sparse\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m, order\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mC\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m   2270\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_raw_data \u001b[39m=\u001b[39m X\n\u001b[0;32m   2272\u001b[0m     \u001b[39m# Handle all the optional arguments, setting default\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\arik_\\anaconda3\\envs\\llm_sem\\Lib\\site-packages\\sklearn\\utils\\validation.py:917\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    915\u001b[0m         array \u001b[39m=\u001b[39m xp\u001b[39m.\u001b[39mastype(array, dtype, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    916\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 917\u001b[0m         array \u001b[39m=\u001b[39m _asarray_with_order(array, order\u001b[39m=\u001b[39;49morder, dtype\u001b[39m=\u001b[39;49mdtype, xp\u001b[39m=\u001b[39;49mxp)\n\u001b[0;32m    918\u001b[0m \u001b[39mexcept\u001b[39;00m ComplexWarning \u001b[39mas\u001b[39;00m complex_warning:\n\u001b[0;32m    919\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    920\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mComplex data not supported\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(array)\n\u001b[0;32m    921\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39mcomplex_warning\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\arik_\\anaconda3\\envs\\llm_sem\\Lib\\site-packages\\sklearn\\utils\\_array_api.py:380\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[1;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[0;32m    378\u001b[0m     array \u001b[39m=\u001b[39m numpy\u001b[39m.\u001b[39marray(array, order\u001b[39m=\u001b[39morder, dtype\u001b[39m=\u001b[39mdtype)\n\u001b[0;32m    379\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 380\u001b[0m     array \u001b[39m=\u001b[39m numpy\u001b[39m.\u001b[39;49masarray(array, order\u001b[39m=\u001b[39;49morder, dtype\u001b[39m=\u001b[39;49mdtype)\n\u001b[0;32m    382\u001b[0m \u001b[39m# At this point array is a NumPy ndarray. We convert it to an array\u001b[39;00m\n\u001b[0;32m    383\u001b[0m \u001b[39m# container that is consistent with the input's namespace.\u001b[39;00m\n\u001b[0;32m    384\u001b[0m \u001b[39mreturn\u001b[39;00m xp\u001b[39m.\u001b[39masarray(array)\n",
      "\u001b[1;31mTypeError\u001b[0m: float() argument must be a string or a real number, not 'dict'"
     ]
    }
   ],
   "source": [
    "tm.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UMAP(angular_rp_forest=True, metric='cosine', min_dist=0, n_components=5, random_state=42, verbose=True)\n",
      "Thu Aug 31 08:12:55 2023 Construct fuzzy simplicial set\n",
      "Thu Aug 31 08:12:55 2023 Finding Nearest Neighbors\n",
      "Thu Aug 31 08:12:55 2023 Building RP forest with 12 trees\n"
     ]
    }
   ],
   "source": [
    "tm.extract_topics(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Topic' object has no attribute 'topic_name'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\arik_\\anaconda3\\envs\\llm_sem\\Lib\\site-packages\\IPython\\core\\formatters.py:708\u001b[0m, in \u001b[0;36mPlainTextFormatter.__call__\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    701\u001b[0m stream \u001b[39m=\u001b[39m StringIO()\n\u001b[0;32m    702\u001b[0m printer \u001b[39m=\u001b[39m pretty\u001b[39m.\u001b[39mRepresentationPrinter(stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose,\n\u001b[0;32m    703\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_width, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnewline,\n\u001b[0;32m    704\u001b[0m     max_seq_length\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_seq_length,\n\u001b[0;32m    705\u001b[0m     singleton_pprinters\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msingleton_printers,\n\u001b[0;32m    706\u001b[0m     type_pprinters\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtype_printers,\n\u001b[0;32m    707\u001b[0m     deferred_pprinters\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdeferred_printers)\n\u001b[1;32m--> 708\u001b[0m printer\u001b[39m.\u001b[39;49mpretty(obj)\n\u001b[0;32m    709\u001b[0m printer\u001b[39m.\u001b[39mflush()\n\u001b[0;32m    710\u001b[0m \u001b[39mreturn\u001b[39;00m stream\u001b[39m.\u001b[39mgetvalue()\n",
      "File \u001b[1;32mc:\\Users\\arik_\\anaconda3\\envs\\llm_sem\\Lib\\site-packages\\IPython\\lib\\pretty.py:410\u001b[0m, in \u001b[0;36mRepresentationPrinter.pretty\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    407\u001b[0m                         \u001b[39mreturn\u001b[39;00m meth(obj, \u001b[39mself\u001b[39m, cycle)\n\u001b[0;32m    408\u001b[0m                 \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mobject\u001b[39m \\\n\u001b[0;32m    409\u001b[0m                         \u001b[39mand\u001b[39;00m \u001b[39mcallable\u001b[39m(\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39m__repr__\u001b[39m\u001b[39m'\u001b[39m)):\n\u001b[1;32m--> 410\u001b[0m                     \u001b[39mreturn\u001b[39;00m _repr_pprint(obj, \u001b[39mself\u001b[39;49m, cycle)\n\u001b[0;32m    412\u001b[0m     \u001b[39mreturn\u001b[39;00m _default_pprint(obj, \u001b[39mself\u001b[39m, cycle)\n\u001b[0;32m    413\u001b[0m \u001b[39mfinally\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\arik_\\anaconda3\\envs\\llm_sem\\Lib\\site-packages\\IPython\\lib\\pretty.py:778\u001b[0m, in \u001b[0;36m_repr_pprint\u001b[1;34m(obj, p, cycle)\u001b[0m\n\u001b[0;32m    776\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"A pprint that just redirects to the normal repr function.\"\"\"\u001b[39;00m\n\u001b[0;32m    777\u001b[0m \u001b[39m# Find newlines and replace them with p.break_()\u001b[39;00m\n\u001b[1;32m--> 778\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mrepr\u001b[39m(obj)\n\u001b[0;32m    779\u001b[0m lines \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39msplitlines()\n\u001b[0;32m    780\u001b[0m \u001b[39mwith\u001b[39;00m p\u001b[39m.\u001b[39mgroup():\n",
      "File \u001b[1;32mc:\\Users\\arik_\\Documents\\Dokumente\\Studium\\Master\\LLM_seminar\\Project\\src\\TopicRepresentation\\TopicRepresentation.py:80\u001b[0m, in \u001b[0;36mTopic.__repr__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__repr__\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[1;32m---> 80\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__str__\u001b[39;49m()\n",
      "File \u001b[1;32mc:\\Users\\arik_\\Documents\\Dokumente\\Studium\\Master\\LLM_seminar\\Project\\src\\TopicRepresentation\\TopicRepresentation.py:70\u001b[0m, in \u001b[0;36mTopic.__str__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__str__\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtopic_idx \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtopic_name \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     71\u001b[0m         \u001b[39mrepr\u001b[39m \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTopic \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mhash\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m     72\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtopic_name \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Topic' object has no attribute 'topic_name'"
     ]
    }
   ],
   "source": [
    "tm.topic_lis[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_sem",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
