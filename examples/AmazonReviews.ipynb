{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\arik_\\anaconda3\\envs\\llm_sem_test7\\Lib\\site-packages\\umap\\distances.py:1063: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "c:\\Users\\arik_\\anaconda3\\envs\\llm_sem_test7\\Lib\\site-packages\\umap\\distances.py:1071: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "c:\\Users\\arik_\\anaconda3\\envs\\llm_sem_test7\\Lib\\site-packages\\umap\\distances.py:1086: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "c:\\Users\\arik_\\anaconda3\\envs\\llm_sem_test7\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\arik_\\anaconda3\\envs\\llm_sem_test7\\Lib\\site-packages\\umap\\umap_.py:660: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "c:\\Users\\arik_\\anaconda3\\envs\\llm_sem_test7\\Lib\\site-packages\\umap\\plot.py:203: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\arik_\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\arik_\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from topicgpt.TopicGPT import TopicGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load api key\n",
    "import os\n",
    "api_key_openai = os.environ.get('OPENAI_API_KEY')\n",
    "\n",
    "import openai\n",
    "\n",
    "openai.organization = \"org-MOfdTrYSke1pXhlAdLXxwDKx\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data from https://www.kaggle.com/datasets/kritanjalijain/amazon-reviews?resource=download\n",
    "\n",
    "review_data = pd.read_csv(\"../Data/AmazonReviews/amazon_review_polarity_csv/train.csv\", header=None) # only use the first 10k reviews of the train set\n",
    "\n",
    "reviews = list(review_data[2])\n",
    "reviews = reviews[:10000] # only consider the first 10k reviews "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm = TopicGPT(\n",
    "    openai_api_key = api_key_openai,\n",
    "    corpus_instruction= \"The Amazon reviews dataset consists of reviews from amazon. The data span a period of 18 years, including 10000 reviews up to March 2013.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm.fit(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm.save_embeddings()  #save the computed embeddings for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm.visualize_clusters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Topic 0: Musical genres and characteristics,\n",
       " Topic 1: Sci-fi TV show.,\n",
       " Topic 2: Film Genres,\n",
       " Topic 3: Paranormal phenomena and UFO sightings,\n",
       " Topic 4: Earbuds and Headsets,\n",
       " Topic 5: Book Review Topics,\n",
       " Topic 6: Gluten-free Cookbook,\n",
       " Topic 7: Air Mattresses,\n",
       " Topic 8: Crime and Investigation.,\n",
       " Topic 9: Printer Troubleshooting,\n",
       " Topic 10: Hiking Footwear,\n",
       " Topic 11: Shapewear,\n",
       " Topic 12: Dance Instruction,\n",
       " Topic 13: Parenting and Education,\n",
       " Topic 14: Electronic Gadgets,\n",
       " Topic 15: Video Games,\n",
       " Topic 16: MP3 Player Issues,\n",
       " Topic 17: Camera Accessories,\n",
       " Topic 18: Power Adapters,\n",
       " Topic 19: Product Quality,\n",
       " Topic 20: Ancient civilizations and anthropology.,\n",
       " Topic 21: Router Connectivity,\n",
       " Topic 22: Technical Issues,\n",
       " Topic 23: Puritanical Society,\n",
       " Topic 24: Sci-fi Space Exploration,\n",
       " Topic 25: Beauty Products,\n",
       " Topic 26: Sexual Vibrators,\n",
       " Topic 27: Home Safety,\n",
       " Topic 28: Product Quality,\n",
       " Topic 29: Customer Service Experience,\n",
       " Topic 30: Textbook Quality,\n",
       " Topic 31: Programming Documentation,\n",
       " Topic 32: Hardware Tools,\n",
       " Topic 33: Product Quality,\n",
       " Topic 34: Educational Toys,\n",
       " Topic 35: Appliances,\n",
       " Topic 36: Kitchenware,\n",
       " Topic 37: Supernatural Witches,\n",
       " Topic 38: Horror Comics,\n",
       " Topic 39: Dystopian society,\n",
       " Topic 40: Emotional Turmoil,\n",
       " Topic 41: Book genres,\n",
       " Topic 42: Economic and Political Critique,\n",
       " Topic 43: Poorly Written Erotica,\n",
       " Topic 44: Dystopian Surveillance State,\n",
       " Topic 45: Experimental Poetry,\n",
       " Topic 46: Formatting Issues,\n",
       " Topic 47: Language Learning Resources,\n",
       " Topic 48: Book genres,\n",
       " Topic 49: Home Improvement,\n",
       " Topic 50: Religious Texts.]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tm.topic_lis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Sep  6 16:28:45 2023 Building and compiling search function\n"
     ]
    }
   ],
   "source": [
    "# load the model if available\n",
    "import pickle\n",
    "with open(\"../Data/SavedTopicRepresentations/TopicGPT_amazonReviews.pkl\", \"rb\") as f:\n",
    "    tm = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what topic 2 is about"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The common topic of the given words is \"Movie Reviews\".\n",
      "\n",
      "Aspects:\n",
      "1. Genre: animated, slasher, noir, zombie, thriller.\n",
      "2. Quality: watchable, unwatchable, dreadful, cheesy, ridiculous.\n",
      "3. Filmmaking: directors, filmmakers, screenwriter, cinematography, filmmaking.\n",
      "4. Audience reaction: scariest, thrilling, hilarious, disappointing, shocking.\n",
      "5. Technical aspects: widescreen, dolby, surround, cinematography, special effects.\n"
     ]
    }
   ],
   "source": [
    "print(tm.topic_lis[2].topic_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT wants to the call the function:  {\n",
      "  \"name\": \"identify_topic_idx\",\n",
      "  \"arguments\": \"{\\n  \\\"query\\\": \\\"Avatar\\\"\\n}\"\n",
      "}\n",
      "GPT wants to the call the function:  {\n",
      "  \"name\": \"get_topic_information\",\n",
      "  \"arguments\": \"{\\n  \\\"topic_idx_lis\\\": [2]\\n}\"\n",
      "}\n",
      "Yes, the movie \"Avatar\" is mentioned in topic 2, which is about film genres. However, the specific context or sentiment of the mention is not provided.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2: '\\n            Topic index: 2\\n            Topic name: Film Genres\\n            Topic description: The common topic of the given words is \"Movie Reviews\".\\n\\nAspects:\\n1. Genre: animated, slasher, noir, zombie, thriller.\\n2. Quality: watchable, unwatchable, dreadful, cheesy, ridiculous.\\n3. Filmmaking: directors, filmmakers, screenwriter, cinematography, filmmaking.\\n4. Audience reaction: scariest, thrilling, hilarious, disappointing, shocking.\\n5. Technical aspects: widescreen, dolby, surround, cinematography, special effects.\\n            Topic topwords: [\\'theaters\\', \\'flicks\\', \\'renting\\', \\'animated\\', \\'robots\\', \\'gang\\', \\'rental\\', \\'filmmakers\\', \\'aerial\\', \\'unrated\\', \\'dubbing\\', \\'credits\\', \\'accents\\', \\'slasher\\', \\'scares\\', \\'noir\\', \\'cop\\', \\'cartoons\\', \\'unwatchable\\', \\'watchable\\', \\'directors\\', \\'porn\\', \\'scariest\\', \\'fights\\', \\'aired\\', \\'adaptation\\', \\'locations\\', \\'wrestling\\', \\'zombie\\', \\'corny\\', \\'watches\\', \\'screenwriter\\', \\'chicks\\', \\'theatrical\\', \\'lesbian\\', \\'sequels\\', \\'villa\\', \\'countryside\\', \\'grabs\\', \\'producer\\', \\'combo\\', \\'redeem\\', \\'trapped\\', \\'marine\\', \\'idiot\\', \\'depicted\\', \\'storylines\\', \\'millions\\', \\'ruin\\', \\'flop\\', \\'thrills\\', \\'claustrophobic\\', \\'caves\\', \\'non-stop\\', \\'blockbuster\\', \\'soldier\\', \\'actresses\\', \\'cinematic\\', \\'breathtaking\\', \\'comedian\\', \\'theatres\\', \\'pilots\\', \\'airplanes\\', \\'widescreen\\', \\'steals\\', \\'grainy\\', \\'acclaimed\\', \\'surround\\', \\'channels\\', \\'innovative\\', \\'buff\\', \\'talents\\', \\'motorcycle\\', \\'soccer\\', \\'courage\\', \\'distracting\\', \\'scenario\\', \\'cult\\', \\'portrays\\', \\'compelled\\', \\'scale\\', \\'segments\\', \\'butt\\', \\'promising\\', \\'cliched\\', \\'sub\\', \\'artsy\\', \\'faces\\', \\'kills\\', \\'rubbish\\', \\'involving\\', \\'vulgar\\', \\'incoherent\\', \\'costumes\\', \\'accent\\', \\'disjointed\\', \\'gory\\', \\'gruesome\\', \\'low-budget\\', \\'dreadful\\', \\'scream\\', \\'host\\', \\'fright\\', \\'action-packed\\', \\'goofy\\', \\'caving\\', \\'driven\\', \\'nude\\', \\'re-make\\', \\'audiences\\', \\'over-the-top\\', \\'filmmaking\\', \\'popcorn\\', \\'aircraft\\', \\'newer\\', \\'sport\\', \\'kicks\\', \\'ventriloquist\\', \\'crying\\', \\'rescue\\', \\'buffs\\', \\'deluxe\\', \\'cheated\\', \\'lackluster\\', \\'dolby\\', \\'strangers\\', \\'pun\\', \\'pitiful\\', \\'adore\\', \\'suffers\\', \\'initially\\', \\'loser\\', \\'arts\\', \\'shocking\\', \\'criminals\\', \\'weapons\\', \\'lion\\', \\'eerie\\', \\'preview\\', \\'youth\\', \\'parent\\', \\'poignant\\', \\'presence\\', \\'decade\\', \\'sympathetic\\', \\'interview\\', \\'achieve\\', \\'jobs\\', \\'motivation\\', \\'blonde\\', \\'poem\\', \\'transitions\\', \\'convincing\\', \\'marketing\\', \\'consumers\\', \\'candy\\', \\'asks\\', \\'walked\\', \\'miscast\\', \\'stunts\\', \\'nasty\\', \\'zombies\\', \\'thrill\\', \\'vomit\\', \\'stupidity\\', \\'stinker\\', \\'freaking\\', \\'disgusting\\', \\'ridiculously\\', \\'farce\\', \\'chills\\', \\'brainless\\', \\'dude\\', \\'naked\\', \\'ratings\\', \\'appalling\\', \\'pretends\\', \\'disgust\\', \\'filming\\', \\'starred\\', \\'punches\\', \\'downhill\\', \\'des\\', \\'sappy\\', \\'breathless\\', \\'screening\\', \\'funniest\\', \\'rude\\', \\'tripe\\', \\'hairy\\', \\'handsome\\', \\'unfunny\\', \\'genuinely\\', \\'fighters\\', \\'spoiled\\', \\'fighter\\', \\'backdrop\\', \\'dogfight\\', \\'planes\\', \\'aviation\\', \\'fast-paced\\', \\'dogfights\\', \\'chasing\\', \\'destruction\\', \\'dramas\\', \\'silent\\', \\'specials\\', \\'westerns\\', \\'permanent\\', \\'blurry\\', \\'blu-rays\\', \\'bikers\\', \\'crazed\\', \\'transfers\\', \\'cue\\', \\'deaf\\', \\'skipping\\', \\'commercials\\', \\'gritty\\', \\'exit\\', \\'jumped\\', \\'continuity\\', \\'robbed\\', \\'knocked\\', \\'sync\\', \\'boxed\\', \\'studios\\', \\'remastered\\', \\'pacing\\', \\'boat\\', \\'headed\\', \\'lonely\\', \\'misfortune\\', \\'marry\\', \\'beg\\', \\'jungle\\', \\'alley\\', \\'suffer\\', \\'victim\\', \\'sympathy\\', \\'wouldnt\\', \\'hurts\\', \\'handled\\', \\'shouting\\', \\'crawl\\', \\'border\\', \\'intent\\', \\'flashbacks\\', \\'viewed\\', \\'spite\\', \\'throat\\', \\'sacrificing\\', \\'ticket\\', \\'painfully\\', \\'passionate\\', \\'martial\\', \\'narration\\', \\'reaction\\', \\'deaths\\', \\'neighbors\\', \\'offend\\', \\'visually\\', \\'perverse\\', \\'realy\\', \\'goal\\', \\'tender\\', \\'portray\\', \\'bullets\\', \\'engaged\\', \\'coverage\\', \\'monotonous\\', \\'unlikely\\', \\'historically\\', \\'banal\\', \\'credibility\\', \\'one-liners\\', \\'depicts\\', \\'caring\\', \\'divorced\\', \\'grave\\', \\'sincere\\', \\'reaches\\', \\'meaningful\\', \\'mild\\', \\'souls\\', \\'downright\\', \\'dramatically\\', \\'involves\\', \\'understatement\\', \\'hates\\', \\'crosses\\', \\'workers\\', \\'interactions\\', \\'overwhelming\\', \\'statues\\', \\'sum\\', \\'photographed\\', \\'ranks\\', \\'aged\\', \\'region\\', \\'post-apocalyptic\\', \\'spoil\\', \\'dud\\', \\'qualities\\', \\'merit\\', \\'borrowed\\', \\'adapted\\', \\'scripts\\', \\'weaker\\', \\'justify\\', \\'purposes\\', \\'coherent\\', \\'gratuitous\\', \\'creature\\', \\'segment\\', \\'whim\\', \\'determine\\', \\'firefighters\\', \\'ok.\\', \\'inaccurate\\', \\'warmth\\', \\'turkey\\', \\'installment\\', \\'picky\\', \\'remotely\\', \\'shell\\', \\'receipt\\', \\'complained\\', \\'phoned\\', \\'cheesey\\', \\'shooting\\', \\'freak\\', \\'cheezy\\', \\'shoots\\', \\'travesty\\', \\'plotless\\', \\'drunk\\', \\'vile\\', \\'spy\\', \\'laughably\\', \\'shameless\\', \\'actors/actresses\\', \\'screams\\', \\'embarrassing\\', \\'rape\\', \\'embarrassed\\', \\'foul\\', \\'half-hour\\', \\'chases\\', \\'retarded\\', \\'crawling\\', \\'spoiler\\', \\'lustful\\', \\'pissed\\', \\'filmmaker\\', \\'disgrace\\', \\'spliced\\', \\'depths\\', \\'uncensored\\', \\'originality\\', \\'twins\\', \\'must-see\\', \\'unreal\\', \\'mansion\\', \\'cameo\\', \\'rendering\\', \\'marvelous\\', \\'comedies\\', \\'crippled\\', \\'comedians\\', \\'marvel\\', \\'fought\\', \\'ghostly\\', \\'spooky\\', \\'bare\\', \\'phenomenal\\', \\'robot\\', \\'underrated\\', \\'beneath\\', \\'comical\\', \\'landing\\', \\'crew\\', \\'heartwarming\\', \\'mute\\', \\'finale\\', \\'teaser\\', \\'airplane\\', \\'nonetheless\\', \\'campy\\', \\'autobots\\', \\'five-star\\', \\'beating\\', \\'reruns\\', \\'marries\\', \\'fond\\', \\'flawless\\', \\'avenger\\', \\'cavalry\\', \\'faded\\', \\'laughter\\', \\'streaming\\', \\'viewings\\', \\'pixelated\\', \\'letterboxed\\', \\'dub\\', \\'biker\\', \\'bluray\\', \\'spiderman\\', \\'beloved\\', \\'beautifull\\', \\'broadcast\\', \\'boxset\\', \\'swinging\\', \\'restored\\', \\'captioning\\', \\'organ\\', \\'skips\\', \\'previews\\', \\'disapointment\\', \\'tickets\\', \\'peanuts\\', \\'holidays\\', \\'insomnia\\', \\'geared\\', \\'suit\\', \\'perfection\\', \\'crisp\\', \\'mesmerizing\\', \\'butter\\', \\'bus\\', \\'vehicle\\', \\'cloying\\', \\'butchered\\', \\'slap\\', \\'spots\\', \\'angles\\', \\'builds\\', \\'tossed\\', \\'facial\\', \\'ripping\\', \\'jumping\\', \\'manufactured\\', \\'glued\\', \\'struck\\', \\'idiots\\', \\'cliff\\', \\'bullet\\', \\'accident\\', \\'performs\\', \\'shakes\\', \\'lossless\\', \\'expired\\', \\'yell\\', \\'grass\\', \\'hurl\\', \\'walks\\', \\'flesh\\', \\'quirky\\', \\'sooo\\', \\'expedition\\', \\'stilted\\', \\'so-so\\', \\'sue\\', \\'cheating\\', \\'regrettably\\', \\'cried\\', \\'alike\\', \\'afterwards\\', \\'escapes\\', \\'headache\\', \\'den\\', \\'shy\\', \\'bisexual\\', \\'effeminate\\', \\'wit\\', \\'suffering\\', \\'cabin\\', \\'minimal\\', \\'riveting\\', \\'brow\\', \\'racism\\', \\'thankfully\\', \\'kinky\\', \\'songwriter\\', \\'mind-numbing\\', \\'leap\\', \\'artifacts\\', \\'attraction\\', \\'enticing\\', \\'mill\\', \\'collectors\\', \\'whiny\\', \\'bickering\\', \\'flawed\\', \\'ton\\', \\'adopted\\', \\'tortured\\', \\'assumed\\', \\'nominated\\', \\'maintain\\']'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tm.pprompt(\"Is the movie Avatar mentioned in topic 2?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check the output, we actually inspect the respective document at index 1498: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not just because of the 3D, but because this is the version where they made an effort to optimize the picture! They released this on blu ray like like avatar was released. First they release the movie without any optimization and special features. Which means the picture looks better than DVD but not the best that blu ray can be.(which means a grainy looking picture that looks like the characters are in a sandstorm and there is a lack of detail that you expect in a blu ray). The they make the limited edition which is made the way a blu ray is supposed to be down. So if you are wondering which one to choose, this is the one you want! All the features with the visuals to boot!\n"
     ]
    }
   ],
   "source": [
    "print(tm.topic_lis[2].documents[1498])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us go own with the analysis. Since it is easy to loose the overview over all the topics, lets find out which one is about books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT wants to the call the function:  {\n",
      "  \"name\": \"identify_topic_idx\",\n",
      "  \"arguments\": \"{\\n  \\\"query\\\": \\\"books\\\"\\n}\"\n",
      "}\n",
      "Topic 5 is about books.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tm.pprompt(\"Which topic is about books?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The common topic of the given words is \"Book Reviews\". \n",
      "\n",
      "Various aspects and sub-topics of this topic include:\n",
      "1. Characters: likable, endearing, mighty, crew, pals\n",
      "2. Storyline: satirical, mythological, strange, satirical, endings\n",
      "3. Writing style: well-crafted, inviting, brilliantly, sarcastic\n",
      "4. Themes: philosophical, allusions, belief, religion, obsession\n",
      "5. Critique: uneven, dissatisfaction, novice, pale, unsuccessful\n"
     ]
    }
   ],
   "source": [
    "print(tm.topic_lis[5].topic_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT wants to the call the function:  {\n",
      "  \"name\": \"knn_search\",\n",
      "  \"arguments\": \"{\\n  \\\"topic_index\\\": 4,\\n  \\\"query\\\": \\\"Harry Potter\\\",\\n  \\\"k\\\": 5\\n}\"\n",
      "}\n",
      "No, there is no mention of Harry Potter in topic 5. The documents that are most closely related to the query \"Harry Potter\" do not mention the topic.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([\"Unable to use. Compartments too tiny and too deep to reach in to get earrings - and I don't have unusually large fingers.\",\n",
       "  \"I wanted an in ear blue tooth headset but couldn't get them to stay in. these made it work!\",\n",
       "  'people should buy headsets to fit these things bc they are so essential..A must have for any headset that will fit them',\n",
       "  'I am am a musician so I use these with earbuds coming off my computer within my headphones which is powered from my Marshall Amplifier. They transfer the sound well and are very well made.',\n",
       "  'The Jabra Eargels are wonderful.. I cannot believe how comfortable, and user friendly they are. Thank you, if not for these, I could not use my bluetooth.Great Merchandise..'],\n",
       " [71, 27, 30, 72, 69])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tm.pprompt(\"Is Harry Potter mentioned in topic 5?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic 0 (\"Musical genres and characteristics\") sounds a bit general and from the visual inspection it seems to contain a lot of documents. So let's break it down a little bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT wants to the call the function:  {\n",
      "  \"name\": \"split_topic_kmeans\",\n",
      "  \"arguments\": \"{\\n  \\\"topic_idx\\\": 0,\\n  \\\"inplace\\\": true\\n}\"\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs completed: 100%| ██████████ 100/100 [00:03]\n",
      "Computing word-topic matrix: 100%|██████████| 1/1 [00:01<00:00,  1.29s/it]\n",
      "Epochs completed: 100%| ██████████ 100/100 [00:01]\n",
      "Epochs completed: 100%| ██████████ 100/100 [00:01]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "tm.pprompt(\"please split topic 0 into subtopics. Do this inplace\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the topic 0 was split into two topics. One on music genres and the other one on remastering of music. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the other hand, topic 0 and topic 2 seem very similar. Let's find out more about the difference between the two: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT wants to the call the function:  {\n",
      "  \"name\": \"get_topic_information\",\n",
      "  \"arguments\": \"{\\n  \\\"topic_idx_lis\\\": [0, 2]\\n}\"\n",
      "}\n",
      "Topic 0 is about sci-fi TV shows, while topic 2 is about paranormal phenomena and UFO sightings. \n",
      "\n",
      "The similarities between the two topics seem to be quite limited based on the available descriptions. Both topics involve speculative elements and may touch on the realm of the unknown. However, the specific sub-topics and aspects mentioned in the descriptions of the topics do not share much overlap.\n",
      "\n",
      "Topic 0, the sci-fi TV show topic, includes sub-topics such as plot and story, setting and characters, originality and creativity, quality and highlights, and viewer experience and opinions. The top words associated with this topic include terms like \"finale\", \"timeline\", \"rewind\", \"hints\", \"galaxy\", \"starship\", \"crew\", and \"originality\".\n",
      "\n",
      "Topic 2, the paranormal and UFO sightings topic, includes sub-topics and aspects such as UFO sightings, unexplained events, belief and speculation, controversy and debunking, and involvement and research. The top words associated with this topic include terms like \"sightings\", \"paranormal\", \"phenomena\", \"UFO\", \"encounters\", \"unexplained\", \"speculation\", and \"believer\".\n",
      "\n",
      "In summary, while both topics may involve elements of speculation and the unknown, they seem to focus on different subject matter – sci-fi TV shows for topic 0 and paranormal phenomena and UFO sightings for topic 2.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: '\\n            Topic index: 0\\n            Topic name: Sci-fi TV show.\\n            Topic description: The common topic of the given words is \"TV shows\" or \"television series\". \\n\\nSub-topics and aspects of this topic include:\\n1. Plot and story: \"finale\", \"timeline\", \"rewind\", \"rehash\", \"hints\"\\n2. Setting and characters: \"galaxy\", \"starship\", \"crew\", \"diehard\"\\n3. Originality and creativity: \"originality\", \"spawned\", \"flawless\"\\n4. Quality and highlights: \"highlights\", \"vice\", \"pacing\", \"woefully\", \"kinks\"\\n5. Viewer experience and opinions: \"farewell\", \"so-so\", \"unintentionally\", \"desperate\", \"on-going\"\\n            Topic topwords: [\\'finale\\', \\'timeline\\', \\'gate\\', \\'rewind\\', \\'rehash\\', \\'hints\\', \\'galaxy\\', \\'served\\', \\'continuation\\', \\'crew\\', \\'starship\\', \\'diehard\\', \\'originality\\', \\'aired\\', \\'orginal\\', \\'spawned\\', \\'flawless\\', \\'highlights\\', \\'vice\\', \\'pacing\\', \\'woefully\\', \\'kinks\\', \\'farewell\\', \\'so-so\\', \\'unintentionally\\', \\'desperate\\', \\'on-going\\', \\'watcher\\', \\'verry\\', \\'crimes\\', \\'arctic\\', \\'wicked\\', \\'stance\\', \\'swearing\\', \\'remaining\\', \\'namely\\', \\'settled\\', \\'tackle\\', \\'travelling\\', \\'babble\\', \\'trial\\', \\'appearances\\', \\'self-absorbed\\', \\'tricks\\', \\'peek\\', \\'eager\\', \\'guard\\', \\'gould\\', \\'span\\', \\'dedicated\\', \\'tying\\', \\'saga\\', \\'intentionally\\', \\'convoluted\\', \\'zing\\', \\'truely\\', \\'devoid\\', \\'dependent\\', \\'infinite\\', \\'parade\\', \\'loyal\\', \\'altered\\', \\'poignant\\', \\'parallel\\', \\'branch\\', \\'profoundly\\', \\'committed\\', \\'resolve\\', \\'gaps\\', \\'trek\\', \\'judgement\\', \\'philosophical\\', \\'entry\\', \\'souls\\', \\'sci\\', \\'versa\\', \\'crammed\\', \\'rocked\\', \\'integrated\\', \\'revisit\\', \\'manipulate\\', \\'owning\\', \\'comeback\\', \\'succesful\\', \\'regulations\\', \\'purposes\\', \\'assuming\\', \\'preparing\\', \\'welcome\\', \\'formed\\', \\'fundamental\\', \\'secured\\', \\'concentrated\\', \\'o.k\\', \\'dinner\\', \\'arriving\\', \\'pleasent\\', \\'prefered\\', \\'satisfaction\\', \\'wrap\\', \\'snacks\\', \\'closure\\', \\'potty\\', \\'installment\\', \\'rainy\\', \\'fluff\\', \\'slipped\\', \\'interplay\\', \\'compression\\', \\'prayer\\', \\'pre-historical\\', \\'positions\\', \\'pre\\', \\'positioned\\', \\'positively\\', \\'praying\\', \\'poses\\', \\'precise\\', \\'ports\\', \\'pose\\', \\'portrays\\', \\'portraying\\', \\'preach\\', \\'preacher\\', \\'portrayals\\', \\'portray\\', \\'preaches\\', \\'preaching\\', \\'preachy\\', \\'preceding\\', \\'portrait\\', \\'precious\\', \\'pre-pregnancy\\', \\'possess\\', \\'post-apocalyptic\\', \\'pray\\', \\'pots\\', \\'practiced\\', \\'potter\\', \\'ppl\\', \\'powershot\\', \\'pound\\', \\'powers\\', \\'pounding\\', \\'pounds\\', \\'powerfully\\', \\'powered\\', \\'powder\\', \\'pour\\', \\'poured\\', \\'poverty\\', \\'potentially\\', \\'potente\\', \\'posture\\', \\'posts\\', \\'praising\\', \\'possibilities\\', \\'possibility\\', \\'pow\\', \\'postage\\', \\'postcard\\', \\'praised\\', \\'possesses\\', \\'prague\\', \\'posted\\', \\'poster\\', \\'posters\\', \\'practicing\\', \\'postings\\', \\'practices\\', \\'postpartum\\', \\'praetorians\\', \\'precisely\\', \\'a/c\\', \\'predator\\', \\'profile\\', \\'proficient\\', \\'professors\\', \\'professionals\\', \\'professionally\\', \\'prof.\\', \\'producto\\', \\'productive\\', \\'productions\\', \\'product.the\\', \\'producing\\', \\'produces\\', \\'producer\\', \\'prodigal\\', \\'proclaimed\\', \\'processes\\', \\'procedures\\', \\'procedure\\', \\'problematic\\', \\'probe\\', \\'prob\\', \\'privilege\\', \\'privacy\\', \\'profit\\', \\'profits\\', \\'programmed\\', \\'programmer\\', \\'propaganda\\', \\'prop\\', \\'proofread\\', \\'pronunciation\\', \\'pronounciation\\', \\'pronounce\\', \\'prongs\\', \\'prone\\', \\'prompted\\', \\'prompt\\', \\'promotion\\', \\'pristine\\', \\'promoting\\', \\'promote\\', \\'promo\\', \\'promising\\', \\'promises\\', \\'prominent\\', \\'prologue\\', \\'proles\\', \\'progressive\\', \\'progresses\\', \\'progressed\\', \\'programmers\\', \\'promotes\\', \\'prisoners\\', \\'prison\\', \\'priority\\', \\'presenting\\', \\'presence\\', \\'prescription\\', \\'prescient\\', \\'preparation\\', \\'prep\\', \\'portions\\', \\'premium\\', \\'premiere\\', \\'prelude\\', \\'prejudice\\', \\'preserve\\', \\'pregnancy\\', \\'preference\\', \\'preface\\', \\'predicts\\', \\'predictions\\', \\'prediction\\', \\'predicting\\', \\'predicted\\', \\'predictability\\', \\'predict\\', \\'predecessors\\', \\'predators\\', \\'prefers\\', \\'precision\\', \\'preserved\\', \\'president\\', \\'principles\\', \\'principle\\', \\'principal\\', \\'princess\\', \\'prince\\', \\'primer\\', \\'primal\\', \\'pride\\', \\'pricey\\', \\'priceless\\', \\'prey\\', \\'preset\\', \\'previews\\', \\'preview\\', \\'prevents\\', \\'prevalent\\', \\'pretension\\', \\'pretends\\', \\'pretend\\', \\'presto\\', \\'prestigious\\', \\'pressured\\', \\'pressing\\', \\'pressed\\', \\'previewed\\', \\'portfolio\\', \\'polidoro\\', \\'port\\', \\'ph.d.\\', \\'peyton\\', \\'pets\\', \\'petite\\', \\'peterson\\', \\'peters\\', \\'pete\\', \\'pesky\\', \\'perverse\\', \\'pervasive\\', \\'persuasion\\', \\'phantom\\', \\'perspectives\\', \\'personaly\\', \\'personalities\\', \\'persona\\', \\'perry\\', \\'perpetual\\', \\'perpetrated\\', \\'pero\\', \\'permanently\\', \\'permanent\\', \\'perks\\', \\'perkins\\', \\'persons\\', \\'periods\\', \\'phase\\', \\'phd\\', \\'pianist\\', \\'piaa\\', \\'physics\\', \\'physically\\', \\'phrasing\\', \\'phrasebook\\', \\'phrase\\', \\'photosmart\\', \\'photographs\\', \\'photographer\\', \\'photographed\\', \\'phases\\', \\'phony\\', \\'phoned\\', \\'phoenix\\', \\'philosophies\\', \\'philosopher\\', \\'philly\\', \\'phillips\\', \\'philadelphia\\', \\'phil\\', \\'phenomenon\\', \\'phenomenal\\', \\'phenomena\\', \\'phones\\', \\'performs\\', \\'performers\\', \\'performer\\', \\'peak\\', \\'peaceful\\', \\'pays\\', \\'payed\\', \\'pause\\', \\'patti\\', \\'pattern\\', \\'patrol\\', \\'patinkin\\', \\'patients\\', \\'patient\\', \\'peaks\\', \\'paths\\', \\'patches\\', \\'patched\\', \\'patch\\', \\'pat\\', \\'pastor\\', \\'paste\\', \\'password\\', \\'passionately\\', \\'passionate\\', \\'passes\\', \\'passage\\', \\'pathos\\', \\'peanuts\\', \\'peck\\', \\'pedal\\', \\'perfection\\', \\'percussion\\', \\'perceptions\\', \\'perception\\', \\'percent\\', \\'perabo\\', \\'peppered\\', \\'pentium\\', \\'pens\\', \\'pennsylvania\\', \\'penguin\\', \\'penetrating\\', \\'penelope\\', \\'pencil\\', \\'penchant\\', \\'pen\\', \\'pellegrino\\', \\'peice\\', \\'pegs\\', \\'peggy\\', \\'peers\\', \\'peeling\\', \\'peel\\', \\'pedestrian\\', \\'pedantic\\', \\'pic\\', \\'portability\\', \\'pickup\\', \\'pics\\', \\'poison\\', \\'pointlessly\\', \\'pointing\\', \\'pointed\\', \\'poets\\', \\'poetic\\', \\'poet\\', \\'poems\\', \\'poem\\', \\'poe\\', \\'podcast\\', \\'poisoning\\', \\'pockets\\', \\'plugs\\', \\'plugging\\', \\'plug-in\\', \\'plotting\\', \\'plotted\\', \\'plotline\\', \\'plotless\\', \\'plot.it\\', \\'plodding\\', \\'plight\\', \\'pliers\\', \\'pny\\', \\'plethora\\', \\'poking\\', \\'polemic\\', \\'porn\\', \\'por\\', \\'population\\', \\'popularity\\', \\'populace\\', \\'pops\\', \\'popping\\', \\'popcorn\\', \\'pop-up\\', \\'poorest\\', \\'poop\\', \\'pole\\', \\'pool\\', \\'ponderous\\', \\'pondering\\', \\'ponder\\', \\'pompous\\', \\'politicians\\', \\'politically\\', \\'polishes\\', \\'polish\\', \\'properties\\', \\'policies\\', \\'policeman\\', \\'pooch\\', \\'plentiful\\', \\'pledge\\', \\'pleasures\\', \\'pitched\\', \\'pitch\\', \\'pita\\', \\'pit\\', \\'pistols\\', \\'pissed\\', \\'piper\\', \\'pipe\\', \\'pioneers\\', \\'pioneer\\', \\'pins\\', \\'pitchshifter\\', \\'pinnacle\\', \\'pinch\\', \\'pilots\\', \\'pilot\\', \\'pills\\', \\'pillows\\', \\'pill\\', \\'pilgrimage\\', \\'pig\\', \\'pierre\\', \\'pieced\\', \\'pie\\', \\'pine\\', \\'pitfalls\\', \\'pitiful\\', \\'pivotal\\', \\'pleasurable\\', \\'pleasing\\', \\'pleasantly\\', \\'playstation\\', \\'playset\\', \\'playlists\\', \\'playlist\\', \\'playboy\\', \\'playback\\', \\'playable\\', \\'plausible\\', \\'platform\\', \\'plates\\', \\'plate\\', \\'plants\\', \\'plant\\', \\'plans\\']',\n",
       " 2: '\\n            Topic index: 2\\n            Topic name: Paranormal phenomena and UFO sightings\\n            Topic description: The common topic of the given words is \"Paranormal Phenomena\". \\n\\nAspects and sub-topics of the topic include:\\n1. UFO Sightings: Encounters, Witnesses, Reported, Sightings\\n2. Unexplained Events: Unexplained, Anecdotes, Incident, Occurrences\\n3. Belief and Speculation: Believer, Speculation, Believed, Possibilities\\n4. Controversy and Debunking: Controversial, Debunked, Dubious, Tendency\\n5. Involvement and Research: Involvement, Researching, Observations, Findings\\n            Topic topwords: [\\'sightings\\', \\'paranormal\\', \\'phenomena\\', \\'ufo\\', \\'encounters\\', \\'unexplained\\', \\'b/c\\', \\'speculation\\', \\'believer\\', \\'relates\\', \\'anecdotes\\', \\'copyright\\', \\'sorely\\', \\'disjointed\\', \\'buff\\', \\'floating\\', \\'visited\\', \\'witnesses\\', \\'stormy\\', \\'admits\\', \\'bin\\', \\'rarity\\', \\'reported\\', \\'ties\\', \\'sighting\\', \\'entitled\\', \\'throws\\', \\'memoir\\', \\'wins\\', \\'stance\\', \\'letdown\\', \\'journalist\\', \\'surrounding\\', \\'hunting\\', \\'incident\\', \\'scenario\\', \\'credibility\\', \\'preview\\', \\'happenings\\', \\'admission\\', \\'reports\\', \\'perpetrated\\', \\'nut\\', \\'delve\\', \\'aunt\\', \\'rampant\\', \\'believed\\', \\'capable\\', \\'touches\\', \\'therein\\', \\'insulting\\', \\'lunatics\\', \\'located\\', \\'pedestrian\\', \\'glance\\', \\'controversial\\', \\'participants\\', \\'conspiracy\\', \\'notably\\', \\'trait\\', \\'possibilities\\', \\'loosely\\', \\'observations\\', \\'involvement\\', \\'hypothesis\\', \\'dubious\\', \\'findings\\', \\'tendency\\', \\'debunked\\', \\'accounts\\', \\'affect\\', \\'positions\\', \\'skeptical\\', \\'pushes\\', \\'mildly\\', \\'sap\\', \\'obsession\\', \\'reporting\\', \\'woo\\', \\'engage\\', \\'assure\\', \\'informs\\', \\'distracted\\', \\'goddesses\\', \\'activity\\', \\'sincere\\', \\'underlying\\', \\'researching\\', \\'applied\\', \\'forgetting\\', \\'behaving\\', \\'theoretical\\', \\'origins\\', \\'occurrences\\', \\'origin\\', \\'plague\\', \\'parallel\\', \\'questionable\\', \\'catagory\\', \\'presenting\\', \\'exhausting\\', \\'generate\\', \\'appearing\\', \\'ramblings\\', \\'mentions\\', \\'documents\\', \\'intrigued\\', \\'unfamiliar\\', \\'approached\\', \\'writings\\', \\'disorganized\\', \\'rang\\', \\'logical\\', \\'concentrated\\', \\'incoherent\\', \\'dross\\', \\'examine\\', \\'understandable\\', \\'thesis\\', \\'adjective\\', \\'teller\\', \\'lengthy\\', \\'blended\\', \\'warns\\', \\'skewed\\', \\'answered\\', \\'reprint\\', \\'scraps\\', \\'storytelling\\', \\'mixture\\', \\'mighty\\', \\'wacky\\', \\'ounce\\', \\'screenwriter\\', \\'goofy\\', \\'wave\\', \\'offbeat\\', \\'phenomenal\\', \\'extraordinary\\', \\'chasing\\', \\'staring\\', \\'aliens\\', \\'dates\\', \\'shocker\\', \\'valid\\', \\'satisfy\\', \\'vehicles\\', \\'ghosts\\', \\'eyed\\', \\'spooky\\', \\'shaped\\', \\'bridge\\', \\'grain\\', \\'gospel\\', \\'composed\\', \\'dose\\', \\'mixes\\', \\'shrink\\', \\'walked\\', \\'ceiling\\', \\'flashlight\\', \\'telephone\\', \\'proles\\', \\'prague\\', \\'programmers\\', \\'possess\\', \\'preach\\', \\'praised\\', \\'praising\\', \\'pray\\', \\'promoting\\', \\'prayer\\', \\'programmer\\', \\'post-apocalyptic\\', \\'praying\\', \\'promotion\\', \\'progresses\\', \\'prompt\\', \\'praetorians\\', \\'postcard\\', \\'pre\\', \\'pre-historical\\', \\'possesses\\', \\'progressed\\', \\'postage\\', \\'prompted\\', \\'prone\\', \\'pre-pregnancy\\', \\'possibility\\', \\'progressive\\', \\'prominent\\', \\'practicing\\', \\'postpartum\\', \\'posts\\', \\'pounds\\', \\'pounding\\', \\'promote\\', \\'pound\\', \\'pour\\', \\'promo\\', \\'posture\\', \\'potty\\', \\'potente\\', \\'potter\\', \\'pots\\', \\'potentially\\', \\'promising\\', \\'prologue\\', \\'postings\\', \\'poverty\\', \\'practices\\', \\'posted\\', \\'practiced\\', \\'ppl\\', \\'powershot\\', \\'powers\\', \\'poured\\', \\'promotes\\', \\'poster\\', \\'promises\\', \\'posters\\', \\'powered\\', \\'powder\\', \\'pow\\', \\'powerfully\\', \\'prisoners\\', \\'profile\\', \\'programmed\\', \\'pretension\\', \\'pretends\\', \\'procedures\\', \\'pretend\\', \\'presto\\', \\'prestigious\\', \\'pressured\\', \\'pressing\\', \\'processes\\', \\'pressed\\', \\'president\\', \\'proclaimed\\', \\'preset\\', \\'preserved\\', \\'prodigal\\', \\'producer\\', \\'preserve\\', \\'produces\\', \\'presence\\', \\'prevalent\\', \\'prescription\\', \\'prevents\\', \\'problematic\\', \\'priority\\', \\'pristine\\', \\'principles\\', \\'principle\\', \\'privacy\\', \\'privilege\\', \\'principal\\', \\'princess\\', \\'prince\\', \\'primer\\', \\'primal\\', \\'pride\\', \\'pricey\\', \\'priceless\\', \\'prob\\', \\'prey\\', \\'previews\\', \\'previewed\\', \\'probe\\', \\'procedure\\', \\'preacher\\', \\'prescient\\', \\'producing\\', \\'predicted\\', \\'predictability\\', \\'predict\\', \\'proficient\\', \\'predecessors\\', \\'predators\\', \\'predator\\', \\'precision\\', \\'precisely\\', \\'precise\\', \\'prison\\', \\'profit\\', \\'profits\\', \\'precious\\', \\'preceding\\', \\'profoundly\\', \\'preachy\\', \\'preaching\\', \\'preaches\\', \\'professors\\', \\'preparing\\', \\'predicting\\', \\'professionals\\', \\'preparation\\', \\'prep\\', \\'premium\\', \\'premiere\\', \\'prelude\\', \\'prejudice\\', \\'pregnancy\\', \\'prefers\\', \\'product.the\\', \\'preference\\', \\'prefered\\', \\'productions\\', \\'productive\\', \\'producto\\', \\'preface\\', \\'predicts\\', \\'predictions\\', \\'prof.\\', \\'professionally\\', \\'prediction\\', \\'a/c\\', \\'pompous\\', \\'positioned\\', \\'philosophies\\', \\'philosophical\\', \\'philosopher\\', \\'philly\\', \\'phillips\\', \\'philadelphia\\', \\'phil\\', \\'phenomenon\\', \\'phd\\', \\'phases\\', \\'phase\\', \\'phoenix\\', \\'phantom\\', \\'peyton\\', \\'pets\\', \\'petite\\', \\'peterson\\', \\'peters\\', \\'pete\\', \\'pesky\\', \\'perverse\\', \\'pervasive\\', \\'persuasion\\', \\'perspectives\\', \\'ph.d.\\', \\'persons\\', \\'phoned\\', \\'phony\\', \\'pilot\\', \\'pills\\', \\'pillows\\', \\'pill\\', \\'pilgrimage\\', \\'pig\\', \\'pierre\\', \\'pieced\\', \\'pie\\', \\'pics\\', \\'picky\\', \\'phones\\', \\'pickup\\', \\'pianist\\', \\'piaa\\', \\'physics\\', \\'physically\\', \\'phrasing\\', \\'phrasebook\\', \\'phrase\\', \\'photosmart\\', \\'photographs\\', \\'photographer\\', \\'photographed\\', \\'pic\\', \\'pilots\\', \\'personaly\\', \\'persona\\', \\'peice\\', \\'pegs\\', \\'peggy\\', \\'peers\\', \\'peeling\\', \\'peel\\', \\'peek\\', \\'pedantic\\', \\'pedal\\', \\'peck\\', \\'peanuts\\', \\'pellegrino\\', \\'peaks\\', \\'peaceful\\', \\'pays\\', \\'payed\\', \\'pause\\', \\'patti\\', \\'pattern\\', \\'patrol\\', \\'patinkin\\', \\'patients\\', \\'patient\\', \\'paths\\', \\'peak\\', \\'personalities\\', \\'pen\\', \\'pencil\\', \\'perry\\', \\'perpetual\\', \\'pero\\', \\'permanently\\', \\'permanent\\', \\'perks\\', \\'perkins\\', \\'periods\\', \\'performs\\', \\'performers\\', \\'performer\\', \\'penchant\\', \\'perfection\\', \\'perceptions\\', \\'perception\\', \\'percent\\', \\'perabo\\', \\'peppered\\', \\'pentium\\', \\'pens\\', \\'pennsylvania\\', \\'penguin\\', \\'penetrating\\', \\'penelope\\', \\'percussion\\', \\'pinch\\', \\'pine\\', \\'pinnacle\\', \\'prongs\\', \\'politicians\\', \\'politically\\', \\'polishes\\', \\'polish\\', \\'polidoro\\', \\'policies\\', \\'policeman\\', \\'polemic\\', \\'pole\\', \\'poking\\', \\'ponder\\', \\'poisoning\\', \\'pointlessly\\', \\'pointing\\', \\'pointed\\', \\'poignant\\', \\'poets\\', \\'poetic\\', \\'poet\\', \\'poems\\', \\'poem\\', \\'poe\\', \\'podcast\\', \\'poison\\', \\'pockets\\', \\'pondering\\', \\'pooch\\', \\'poses\\', \\'pose\\', \\'ports\\', \\'portrays\\', \\'portraying\\', \\'portrayals\\', \\'portray\\', \\'portrait\\', \\'portions\\', \\'portfolio\\', \\'portability\\', \\'ponderous\\', \\'port\\', \\'por\\', \\'population\\', \\'popularity\\', \\'populace\\', \\'pops\\', \\'popping\\', \\'popcorn\\', \\'pop-up\\', \\'poorest\\', \\'poop\\', \\'pool\\', \\'porn\\', \\'pny\\', \\'plugs\\', \\'plugging\\', \\'plant\\', \\'plans\\', \\'planned\\', \\'planets\\', \\'planes\\', \\'plainly\\', \\'placing\\', \\'placement\\', \\'pixelated\\', \\'pivotal\\', \\'pitiful\\', \\'plants\\']'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tm.pprompt(\"What are the differences and similarities of topic 0 and topic 2?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The topics seem fairly related, so we can merge them into one topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT wants to the call the function:  {\n",
      "  \"name\": \"combine_topics\",\n",
      "  \"arguments\": \"{\\n  \\\"topic_idx_lis\\\": [0, 2],\\n  \\\"inplace\\\": true\\n}\"\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs completed: 100%| ██████████ 100/100 [00:01]\n",
      "Computing word-topic matrix: 100%|██████████| 1/1 [00:02<00:00,  2.39s/it]\n"
     ]
    },
    {
     "ename": "SystemError",
     "evalue": "CPUDispatcher(<function NNDescent._init_search_function.<locals>.search_closure at 0x0000026C0365B600>) returned a result with an exception set",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\arik_\\anaconda3\\envs\\llm_sem\\Lib\\site-packages\\numba\\core\\serialize.py:30\u001b[0m, in \u001b[0;36m_numba_unpickle\u001b[1;34m(address, bytedata, hashed)\u001b[0m\n\u001b[0;32m     27\u001b[0m _unpickled_memo \u001b[39m=\u001b[39m {}\n\u001b[1;32m---> 30\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_numba_unpickle\u001b[39m(address, bytedata, hashed):\n\u001b[0;32m     31\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Used by `numba_unpickle` from _helperlib.c\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \n\u001b[0;32m     33\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[39m        unpickled object\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mSystemError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;31mSystemError\u001b[0m: <function _numba_unpickle at 0x0000026C6F2536A0> returned a result with an exception set",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mSystemError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m tm\u001b[39m.\u001b[39;49mpprompt(\u001b[39m\"\u001b[39;49m\u001b[39mplease merge topic 0 and topic 2. Do this inplace\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\users\\arik_\\documents\\dokumente\\studium\\master\\llm_seminar\\project\\src\\topicgpt\\TopicGPT.py:330\u001b[0m, in \u001b[0;36mTopicGPT.pprompt\u001b[1;34m(self, query, return_function_result)\u001b[0m\n\u001b[0;32m    320\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpprompt\u001b[39m(\u001b[39mself\u001b[39m, query:\u001b[39mstr\u001b[39m, return_function_result: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mobject\u001b[39m:\n\u001b[0;32m    321\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    322\u001b[0m \u001b[39m    This function prompts the model with the query and prints the answer.\u001b[39;00m\n\u001b[0;32m    323\u001b[0m \u001b[39m    params:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[39m        function_result: The result of the function call.    \u001b[39;00m\n\u001b[0;32m    328\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 330\u001b[0m     answer, function_result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprompt(query)\n\u001b[0;32m    332\u001b[0m     \u001b[39mprint\u001b[39m(answer)\n\u001b[0;32m    334\u001b[0m     \u001b[39mif\u001b[39;00m return_function_result:\n",
      "File \u001b[1;32mc:\\users\\arik_\\documents\\dokumente\\studium\\master\\llm_seminar\\project\\src\\topicgpt\\TopicGPT.py:311\u001b[0m, in \u001b[0;36mTopicGPT.prompt\u001b[1;34m(self, query)\u001b[0m\n\u001b[0;32m    301\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprompt\u001b[39m(\u001b[39mself\u001b[39m, query: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m (\u001b[39mstr\u001b[39m, \u001b[39mobject\u001b[39m):\n\u001b[0;32m    302\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    303\u001b[0m \u001b[39m    This function prompts the model with the query. Please Have a look at the TopicPrompting class for more details on available functions for prompting the model.\u001b[39;00m\n\u001b[0;32m    304\u001b[0m \u001b[39m    params:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    308\u001b[0m \u001b[39m        function_result: The result of the function call.\u001b[39;00m\n\u001b[0;32m    309\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 311\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtopic_prompting\u001b[39m.\u001b[39;49mgeneral_prompt(query)\n\u001b[0;32m    313\u001b[0m     answer \u001b[39m=\u001b[39m result[\u001b[39m0\u001b[39m][\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mchoices\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mmessage\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m    314\u001b[0m     function_result \u001b[39m=\u001b[39m result[\u001b[39m1\u001b[39m]\n",
      "File \u001b[1;32mc:\\users\\arik_\\documents\\dokumente\\studium\\master\\llm_seminar\\project\\src\\topicgpt\\TopicPrompting.py:1127\u001b[0m, in \u001b[0;36mTopicPrompting.general_prompt\u001b[1;34m(self, prompt, n_tries)\u001b[0m\n\u001b[0;32m   1125\u001b[0m function_to_call \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunctionNames2Functions[function_name]\n\u001b[0;32m   1126\u001b[0m function_args \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mloads(function_call[\u001b[39m\"\u001b[39m\u001b[39marguments\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m-> 1127\u001b[0m function_response \u001b[39m=\u001b[39m function_to_call(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfunction_args)\n\u001b[0;32m   1128\u001b[0m function_response_json \u001b[39m=\u001b[39m function_response[\u001b[39m0\u001b[39m]\n\u001b[0;32m   1129\u001b[0m function_response_return_output \u001b[39m=\u001b[39m function_response[\u001b[39m1\u001b[39m]\n",
      "File \u001b[1;32mc:\\users\\arik_\\documents\\dokumente\\studium\\master\\llm_seminar\\project\\src\\topicgpt\\TopicPrompting.py:1022\u001b[0m, in \u001b[0;36mTopicPrompting._combine_topics_openai\u001b[1;34m(self, topic_idx_lis, inplace)\u001b[0m\n\u001b[0;32m   1013\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_combine_topics_openai\u001b[39m(\u001b[39mself\u001b[39m, topic_idx_lis: \u001b[39mlist\u001b[39m[\u001b[39mint\u001b[39m], inplace \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m (json, \u001b[39mlist\u001b[39m[Topic]):\n\u001b[0;32m   1014\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1015\u001b[0m \u001b[39m    A version of the combine_topics function that returns a json file to be used with the openai API\u001b[39;00m\n\u001b[0;32m   1016\u001b[0m \u001b[39m    params:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1020\u001b[0m \u001b[39m        json object to be used with the openai API, also returns the new topics\u001b[39;00m\n\u001b[0;32m   1021\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1022\u001b[0m     new_topics \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcombine_topics(topic_idx_lis, inplace)\n\u001b[0;32m   1023\u001b[0m     json_obj \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mdumps({\n\u001b[0;32m   1024\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mnew topics\u001b[39m\u001b[39m\"\u001b[39m: [topic\u001b[39m.\u001b[39mto_dict() \u001b[39mfor\u001b[39;00m topic \u001b[39min\u001b[39;00m new_topics][\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m   1025\u001b[0m     })\n\u001b[0;32m   1026\u001b[0m     \u001b[39mreturn\u001b[39;00m json_obj, new_topics\n",
      "File \u001b[1;32mc:\\users\\arik_\\documents\\dokumente\\studium\\master\\llm_seminar\\project\\src\\topicgpt\\TopicPrompting.py:715\u001b[0m, in \u001b[0;36mTopicPrompting.combine_topics\u001b[1;34m(self, topic_idx_lis, inplace)\u001b[0m\n\u001b[0;32m    711\u001b[0m     new_topic_document_embeddings_hd\u001b[39m.\u001b[39mappend(topic\u001b[39m.\u001b[39mdocument_embeddings_hd)\n\u001b[0;32m    713\u001b[0m new_topic_document_embeddings_hd \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mconcatenate(new_topic_document_embeddings_hd, axis \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m)\n\u001b[1;32m--> 715\u001b[0m new_topic \u001b[39m=\u001b[39m extract_and_describe_topic_cos_sim(\n\u001b[0;32m    716\u001b[0m     documents_topic \u001b[39m=\u001b[39;49m new_topic_docs,\n\u001b[0;32m    717\u001b[0m     document_embeddings_topic \u001b[39m=\u001b[39;49m new_topic_document_embeddings_hd,\n\u001b[0;32m    718\u001b[0m     words_topic \u001b[39m=\u001b[39;49m new_topic_words,\n\u001b[0;32m    719\u001b[0m     vocab_embeddings \u001b[39m=\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvocab_embeddings,\n\u001b[0;32m    720\u001b[0m     umap_mapper \u001b[39m=\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtopic_lis[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49mumap_mapper,\n\u001b[0;32m    721\u001b[0m     enhancer\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menhancer,\n\u001b[0;32m    722\u001b[0m     n_topwords \u001b[39m=\u001b[39;49m \u001b[39m2000\u001b[39;49m\n\u001b[0;32m    723\u001b[0m )\n\u001b[0;32m    725\u001b[0m new_topic\u001b[39m.\u001b[39mtopic_idx \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtopic_lis) \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    726\u001b[0m new_topic_lis \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtopic_lis\u001b[39m.\u001b[39mcopy()\n",
      "File \u001b[1;32mc:\\users\\arik_\\documents\\dokumente\\studium\\master\\llm_seminar\\project\\src\\topicgpt\\TopicRepresentation.py:565\u001b[0m, in \u001b[0;36mextract_and_describe_topic_cos_sim\u001b[1;34m(documents_topic, document_embeddings_topic, words_topic, vocab_embeddings, umap_mapper, enhancer, n_topwords, n_topwords_description)\u001b[0m\n\u001b[0;32m    542\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[0;32m    543\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mextract_and_describe_topic_cos_sim\u001b[39m(documents_topic: \u001b[39mlist\u001b[39m[\u001b[39mstr\u001b[39m], \n\u001b[0;32m    544\u001b[0m                   document_embeddings_topic: np\u001b[39m.\u001b[39mndarray, \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    549\u001b[0m                   n_topwords: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39m2000\u001b[39m, \n\u001b[0;32m    550\u001b[0m                   n_topwords_description \u001b[39m=\u001b[39m \u001b[39m500\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Topic:\n\u001b[0;32m    551\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    552\u001b[0m \u001b[39m    create a topic object from the given documents and embeddings. i.e. compute the centroid and the top-words. Only use cosine-similarity for top-word extraction. Describe and name the topic with the given enhancer object\u001b[39;00m\n\u001b[0;32m    553\u001b[0m \u001b[39m    params:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    563\u001b[0m \u001b[39m        Topic object\u001b[39;00m\n\u001b[0;32m    564\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 565\u001b[0m     topic \u001b[39m=\u001b[39m extract_topic_cos_sim(documents_topic, document_embeddings_topic, words_topic, vocab_embeddings, umap_mapper, n_topwords)\n\u001b[0;32m    566\u001b[0m     topic \u001b[39m=\u001b[39m describe_and_name_topics([topic], enhancer, \u001b[39m\"\u001b[39m\u001b[39mcosine_similarity\u001b[39m\u001b[39m\"\u001b[39m, n_topwords_description)[\u001b[39m0\u001b[39m]\n\u001b[0;32m    567\u001b[0m     \u001b[39mreturn\u001b[39;00m topic\n",
      "File \u001b[1;32mc:\\users\\arik_\\documents\\dokumente\\studium\\master\\llm_seminar\\project\\src\\topicgpt\\TopicRepresentation.py:512\u001b[0m, in \u001b[0;36mextract_topic_cos_sim\u001b[1;34m(documents_topic, document_embeddings_topic, words_topic, vocab_embeddings, umap_mapper, n_topwords)\u001b[0m\n\u001b[0;32m    510\u001b[0m word_topic_mat \u001b[39m=\u001b[39m extractor\u001b[39m.\u001b[39mcompute_word_topic_mat(documents_topic, words_topic, labels, consider_outliers \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m)  \u001b[39m# compute the word-topic matrix of the corpus\u001b[39;00m\n\u001b[0;32m    511\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mcosine_similarity\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m topword_extraction_methods:\n\u001b[1;32m--> 512\u001b[0m     cosine_topwords, cosine_dict \u001b[39m=\u001b[39m extractor\u001b[39m.\u001b[39;49mextract_topwords_centroid_similarity(word_topic_mat \u001b[39m=\u001b[39;49m word_topic_mat, vocab \u001b[39m=\u001b[39;49m words_topic, vocab_embedding_dict \u001b[39m=\u001b[39;49m vocab_embeddings, centroid_dict\u001b[39m=\u001b[39;49m {\u001b[39m0\u001b[39;49m: centroid_ld}, umap_mapper \u001b[39m=\u001b[39;49m umap_mapper, top_n_words \u001b[39m=\u001b[39;49m n_topwords, reduce_vocab_embeddings \u001b[39m=\u001b[39;49m \u001b[39mTrue\u001b[39;49;00m, reduce_centroid_embeddings \u001b[39m=\u001b[39;49m \u001b[39mFalse\u001b[39;49;00m, consider_outliers \u001b[39m=\u001b[39;49m \u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m    516\u001b[0m top_words \u001b[39m=\u001b[39m {\n\u001b[0;32m    517\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcosine_similarity\u001b[39m\u001b[39m\"\u001b[39m: cosine_topwords \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mcosine_similarity\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m topword_extraction_methods \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    518\u001b[0m }\n\u001b[0;32m    519\u001b[0m top_word_scores \u001b[39m=\u001b[39m {\n\u001b[0;32m    520\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcosine_similarity\u001b[39m\u001b[39m\"\u001b[39m: cosine_dict \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mcosine_similarity\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m topword_extraction_methods \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    521\u001b[0m }\n",
      "File \u001b[1;32mc:\\users\\arik_\\documents\\dokumente\\studium\\master\\llm_seminar\\project\\src\\topicgpt\\ExtractTopWords.py:381\u001b[0m, in \u001b[0;36mExtractTopWords.extract_topwords_centroid_similarity\u001b[1;34m(self, word_topic_mat, vocab, vocab_embedding_dict, centroid_dict, umap_mapper, top_n_words, reduce_vocab_embeddings, reduce_centroid_embeddings, consider_outliers)\u001b[0m\n\u001b[0;32m    363\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mextract_topwords_centroid_similarity\u001b[39m(\u001b[39mself\u001b[39m, word_topic_mat: np\u001b[39m.\u001b[39mndarray, vocab: \u001b[39mlist\u001b[39m[\u001b[39mstr\u001b[39m], vocab_embedding_dict: \u001b[39mdict\u001b[39m, centroid_dict: \u001b[39mdict\u001b[39m, umap_mapper: umap\u001b[39m.\u001b[39mUMAP, top_n_words: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m, reduce_vocab_embeddings: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m, reduce_centroid_embeddings: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m, consider_outliers: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m (\u001b[39mdict\u001b[39m, np\u001b[39m.\u001b[39mndarray):\n\u001b[0;32m    364\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    365\u001b[0m \u001b[39m    Extract the top-words for each cluster by computing the cosine similarity of the words that occur in the corpus to the centroid of the cluster\u001b[39;00m\n\u001b[0;32m    366\u001b[0m \u001b[39m    params: \u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    379\u001b[0m \u001b[39m        np.ndarray, cosine similarity of each word in the vocab to each centroid. has shape (len(vocab), len(centroid_dict) - 1)\u001b[39;00m\n\u001b[0;32m    380\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 381\u001b[0m     similarity_mat \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_embedding_similarity_centroids(vocab, vocab_embedding_dict, umap_mapper, centroid_dict, reduce_vocab_embeddings, reduce_centroid_embeddings)\n\u001b[0;32m    382\u001b[0m     top_words \u001b[39m=\u001b[39m {}\n\u001b[0;32m    383\u001b[0m     top_word_scores \u001b[39m=\u001b[39m {}\n",
      "File \u001b[1;32mc:\\users\\arik_\\documents\\dokumente\\studium\\master\\llm_seminar\\project\\src\\topicgpt\\ExtractTopWords.py:356\u001b[0m, in \u001b[0;36mExtractTopWords.compute_embedding_similarity_centroids\u001b[1;34m(self, vocab, vocab_embedding_dict, umap_mapper, centroid_dict, reduce_vocab_embeddings, reduce_centroid_embeddings)\u001b[0m\n\u001b[0;32m    354\u001b[0m     vocab_arr[i] \u001b[39m=\u001b[39m vocab_embedding_dict[word]\n\u001b[0;32m    355\u001b[0m \u001b[39mif\u001b[39;00m reduce_vocab_embeddings:\n\u001b[1;32m--> 356\u001b[0m     vocab_arr \u001b[39m=\u001b[39m umap_mapper\u001b[39m.\u001b[39;49mtransform(vocab_arr)\n\u001b[0;32m    358\u001b[0m vocab_arr \u001b[39m=\u001b[39m vocab_arr \u001b[39m/\u001b[39m np\u001b[39m.\u001b[39mlinalg\u001b[39m.\u001b[39mnorm(vocab_arr, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,\u001b[39m1\u001b[39m)\n\u001b[0;32m    360\u001b[0m similarity \u001b[39m=\u001b[39m vocab_arr \u001b[39m@\u001b[39m centroid_arr\u001b[39m.\u001b[39mT \u001b[39m# cosine similarity\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\arik_\\anaconda3\\envs\\llm_sem\\Lib\\site-packages\\umap\\umap_.py:2896\u001b[0m, in \u001b[0;36mUMAP.transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m   2894\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   2895\u001b[0m     epsilon \u001b[39m=\u001b[39m \u001b[39m0.24\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_knn_search_index\u001b[39m.\u001b[39m_angular_trees \u001b[39melse\u001b[39;00m \u001b[39m0.12\u001b[39m\n\u001b[1;32m-> 2896\u001b[0m     indices, dists \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_knn_search_index\u001b[39m.\u001b[39;49mquery(\n\u001b[0;32m   2897\u001b[0m         X, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_neighbors, epsilon\u001b[39m=\u001b[39;49mepsilon\n\u001b[0;32m   2898\u001b[0m     )\n\u001b[0;32m   2900\u001b[0m dists \u001b[39m=\u001b[39m dists\u001b[39m.\u001b[39mastype(np\u001b[39m.\u001b[39mfloat32, order\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mC\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   2901\u001b[0m \u001b[39m# Remove any nearest neighbours who's distances are greater than our disconnection_distance\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\arik_\\anaconda3\\envs\\llm_sem\\Lib\\site-packages\\pynndescent\\pynndescent_.py:1686\u001b[0m, in \u001b[0;36mNNDescent.query\u001b[1;34m(self, query_data, k, epsilon)\u001b[0m\n\u001b[0;32m   1683\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_search_function()\n\u001b[0;32m   1685\u001b[0m     query_data \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(query_data)\u001b[39m.\u001b[39mastype(np\u001b[39m.\u001b[39mfloat32, order\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mC\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> 1686\u001b[0m     indices, dists, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_search_function(\n\u001b[0;32m   1687\u001b[0m         query_data, k, epsilon, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_visited, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msearch_rng_state\n\u001b[0;32m   1688\u001b[0m     )\n\u001b[0;32m   1689\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1690\u001b[0m     \u001b[39m# Sparse case\u001b[39;00m\n\u001b[0;32m   1691\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m_search_function\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "\u001b[1;31mSystemError\u001b[0m: CPUDispatcher(<function NNDescent._init_search_function.<locals>.search_closure at 0x0000026C0365B600>) returned a result with an exception set"
     ]
    }
   ],
   "source": [
    "tm.pprompt(\"please merge topic 0 and topic 2. Do this inplace\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might be interested in a topic about reviews of TVs. Let' see if there is one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT wants to the call the function:  {\n",
      "  \"name\": \"identify_topic_idx\",\n",
      "  \"arguments\": \"{\\n\\\"query\\\": \\\"TVs\\\"\\n}\"\n",
      "}\n",
      "I'm sorry, but I couldn't find a specific topic about TVs in the provided dataset.\n"
     ]
    }
   ],
   "source": [
    "tm.pprompt(\"Is there a topic about TVs?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There doesn't seem to be a topic about TVs, so we can create one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT wants to the call the function:  {\n",
      "  \"name\": \"add_new_topic_keyword\",\n",
      "  \"arguments\": \"{\\n  \\\"keyword\\\": \\\"TVs\\\"\\n}\"\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs completed: 100%| ██████████ 100/100 [00:01]\n",
      "Epochs completed: 100%| ██████████ 100/100 [00:01]\n",
      "Computing word-topic matrix: 100%|██████████| 53/53 [00:08<00:00,  6.18it/s]\n",
      "Epochs completed: 100%| ██████████ 100/100 [00:14]\n",
      " 43%|████▎     | 23/53 [05:57<04:58,  9.94s/it] "
     ]
    }
   ],
   "source": [
    "tm.pprompt(\"Please create a new topic that encompasses documents about TVs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, plagiarism is seems to be the main reason for very bad sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The common topic of the given words is \"Product Reviews and Evaluation\". \\n\\nVarious aspects and sub-topics of this topic include:\\n1. Evaluation of product quality: \"faults\", \"merits\", \"precision\", \"obtained\", \"premium\"\\n2. Evaluation of product performance: \"predicts\", \"prediction\", \"promising\", \"progresses\", \"progressive\"\\n3. Evaluation of product features: \"portability\", \"powerful\", \"pricey\", \"pound\", \"popularity\"\\n4. Evaluation of product value: \"priceless\", \"profit\", \"profits\", \"promotes\", \"promotion\"\\n5. Evaluation of product reliability: \"protected\", \"protection\", \"proven\", \"profoundly\", \"profound\"'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tm.topic_lis[51].topic_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "On the other hand, topics 68 and 69 seem very similar, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT wants to the call the function:  {\n",
      "  \"name\": \"combine_topics\",\n",
      "  \"arguments\": \"{\\n  \\\"topic_idx_lis\\\": [68, 69],\\n  \\\"inplace\\\": true\\n}\"\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs completed: 100%| ██████████ 100/100 [00:01]\n",
      "Computing word-topic matrix: 100%|██████████| 1/1 [00:00<00:00, 10.32it/s]\n",
      "Epochs completed: 100%| ██████████ 30/30 [00:02]\n",
      "Epochs completed: 100%| ██████████ 100/100 [00:01]\n",
      "100%|██████████| 1/1 [00:06<00:00,  6.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics 68 and 69 have been combined into a new topic, identified as Topic 73: \"Fantasy Literature\". This topic encompasses various aspects and sub-topics, including creatures like dwarves, elves, trolls, goblins, and wizards, as well as magical elements such as spells, enchanting, fables, and magical creatures. Additionally, the topic includes elements of adventure and exploration, such as tracking, thrills, and exploring, as well as characters and storytelling, including imaginative characters, castles, stories, and narrators. Furthermore, the topic may also involve critiques and opinions related to imaginative literature, including words like unimaginative, overused, disliked, and unoriginal. Please note that these sub-topics are inferred based on the provided words and may not represent the complete range of topics in the corpus. \n",
      "\n",
      "used function call: {\n",
      "  \"name\": \"combine_topics\",\n",
      "  \"arguments\": \"{\\n  \\\"topic_idx_lis\\\": [68, 69],\\n  \\\"inplace\\\": true\\n}\"\n",
      "} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "tm.pprompt(\"please combine topics 68 and 69. Do this inplace\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Topic 0: Music genres,\n",
       " Topic 1: Diapers,\n",
       " Topic 2: Superhero Movies,\n",
       " Topic 3: Movies and Filmmaking,\n",
       " Topic 4: Pest Control,\n",
       " Topic 5: Dystopian Society,\n",
       " Topic 6: Cooking,\n",
       " Topic 7: Classic Literature,\n",
       " Topic 8: Survival and Savagery.,\n",
       " Topic 9: Video Games,\n",
       " Topic 10: Printer-related issues,\n",
       " Topic 11: Camera Accessories,\n",
       " Topic 12: Massage Therapy,\n",
       " Topic 13: Adventure and Mystery.,\n",
       " Topic 14: Product Reviews,\n",
       " Topic 15: Health supplements,\n",
       " Topic 16: Pet Treats,\n",
       " Topic 17: Whaling and Survival.,\n",
       " Topic 18: Inflatable Airbeds,\n",
       " Topic 19: Product Defects,\n",
       " Topic 20: Prehistoric hunting,\n",
       " Topic 21: Book Reviews,\n",
       " Topic 22: Coffee and Beverage,\n",
       " Topic 23: Coffee Brewing Equipment,\n",
       " Topic 24: Product reviews,\n",
       " Topic 25: Plumbing Accessories,\n",
       " Topic 26: Appliances,\n",
       " Topic 27: Kitchen Appliances,\n",
       " Topic 28: Kitchen Utensils,\n",
       " Topic 29: Relationships and Dating,\n",
       " Topic 30: Bicycle Accessories,\n",
       " Topic 31: Product Features,\n",
       " Topic 32: Children's Toys,\n",
       " Topic 33: baby products,\n",
       " Topic 34: Product Recommendation,\n",
       " Topic 35: Hardware Tools,\n",
       " Topic 36: Electronics connectivity,\n",
       " Topic 37: Networking and Connectivity,\n",
       " Topic 38: Flashlights,\n",
       " Topic 39: Audio/Video Connectivity,\n",
       " Topic 40: Shiny Objects,\n",
       " Topic 41: Shoes,\n",
       " Topic 42: Clothing quality and fit.,\n",
       " Topic 43: Gardening,\n",
       " Topic 44: Product Reviews,\n",
       " Topic 45: Electronic Accessories,\n",
       " Topic 46: Electronic Devices,\n",
       " Topic 47: Language Learning,\n",
       " Topic 48: Audio Equipment,\n",
       " Topic 49: Electronics,\n",
       " Topic 50: Online Shopping Experience,\n",
       " Topic 51: Defective electronic devices.,\n",
       " Topic 52: Book Reviews,\n",
       " Topic 53: Book Formatting,\n",
       " Topic 54: Investing in Stocks,\n",
       " Topic 55: Book genres,\n",
       " Topic 56: Programming Books,\n",
       " Topic 57: Book Reviews,\n",
       " Topic 58: Art and Design,\n",
       " Topic 59: Legal Proceedings,\n",
       " Topic 60: book genres,\n",
       " Topic 61: Book Criticisms,\n",
       " Topic 62: Dystopian literature,\n",
       " Topic 63: Religious Controversies,\n",
       " Topic 64: Self-help and spirituality.,\n",
       " Topic 65: Christianity and Prayer,\n",
       " Topic 66: book genres,\n",
       " Topic 67: Book genres,\n",
       " Topic 68: Fantasy Books,\n",
       " Topic 69: Nature and Adventure,\n",
       " Topic 70: Historical Events,\n",
       " Topic 71: Family and Childhood,\n",
       " Topic 72: Books and Reading,\n",
       " Topic 73: Fantasy Literature]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tm.topic_lis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_sem",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
