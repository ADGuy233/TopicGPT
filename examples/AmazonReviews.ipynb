{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\arik_\\anaconda3\\envs\\llm_sem\\Lib\\site-packages\\umap\\distances.py:1063: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "c:\\Users\\arik_\\anaconda3\\envs\\llm_sem\\Lib\\site-packages\\umap\\distances.py:1071: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "c:\\Users\\arik_\\anaconda3\\envs\\llm_sem\\Lib\\site-packages\\umap\\distances.py:1086: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "c:\\Users\\arik_\\anaconda3\\envs\\llm_sem\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\arik_\\anaconda3\\envs\\llm_sem\\Lib\\site-packages\\umap\\umap_.py:660: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "c:\\Users\\arik_\\anaconda3\\envs\\llm_sem\\Lib\\site-packages\\umap\\plot.py:203: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\arik_\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\arik_\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from topicgpt.TopicGPT import TopicGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load api key\n",
    "import os\n",
    "api_key_openai = os.environ.get('OPENAI_API_KEY2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data from https://www.kaggle.com/datasets/kritanjalijain/amazon-reviews?resource=download\n",
    "\n",
    "review_data = pd.read_csv(\"../Data/AmazonReviews/amazon_review_polarity_csv/train.csv\", header=None) # only use the first 10k reviews of the train set\n",
    "\n",
    "reviews = list(review_data[2])\n",
    "reviews = reviews[:10000] # only consider the first 10k reviews "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm = TopicGPT(\n",
    "    openai_api_key = api_key_openai,\n",
    "    corpus_instruction= \"The Amazon reviews dataset consists of reviews from amazon. The data span a period of 18 years, including 10000 reviews up to March 2013.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm.fit(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm.save_embeddings()  #save the computed embeddings for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm.visualize_clusters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Topic 0: Musical genres and characteristics,\n",
       " Topic 1: Sci-fi TV show.,\n",
       " Topic 2: Film Genres,\n",
       " Topic 3: Paranormal phenomena and UFO sightings,\n",
       " Topic 4: Earbuds and Headsets,\n",
       " Topic 5: Book Review Topics,\n",
       " Topic 6: Gluten-free Cookbook,\n",
       " Topic 7: Air Mattresses,\n",
       " Topic 8: Crime and Investigation.,\n",
       " Topic 9: Printer Troubleshooting,\n",
       " Topic 10: Hiking Footwear,\n",
       " Topic 11: Shapewear,\n",
       " Topic 12: Dance Instruction,\n",
       " Topic 13: Parenting and Education,\n",
       " Topic 14: Electronic Gadgets,\n",
       " Topic 15: Video Games,\n",
       " Topic 16: MP3 Player Issues,\n",
       " Topic 17: Camera Accessories,\n",
       " Topic 18: Power Adapters,\n",
       " Topic 19: Product Quality,\n",
       " Topic 20: Ancient civilizations and anthropology.,\n",
       " Topic 21: Router Connectivity,\n",
       " Topic 22: Technical Issues,\n",
       " Topic 23: Puritanical Society,\n",
       " Topic 24: Sci-fi Space Exploration,\n",
       " Topic 25: Beauty Products,\n",
       " Topic 26: Sexual Vibrators,\n",
       " Topic 27: Home Safety,\n",
       " Topic 28: Product Quality,\n",
       " Topic 29: Customer Service Experience,\n",
       " Topic 30: Textbook Quality,\n",
       " Topic 31: Programming Documentation,\n",
       " Topic 32: Hardware Tools,\n",
       " Topic 33: Product Quality,\n",
       " Topic 34: Educational Toys,\n",
       " Topic 35: Appliances,\n",
       " Topic 36: Kitchenware,\n",
       " Topic 37: Supernatural Witches,\n",
       " Topic 38: Horror Comics,\n",
       " Topic 39: Dystopian society,\n",
       " Topic 40: Emotional Turmoil,\n",
       " Topic 41: Book genres,\n",
       " Topic 42: Economic and Political Critique,\n",
       " Topic 43: Poorly Written Erotica,\n",
       " Topic 44: Dystopian Surveillance State,\n",
       " Topic 45: Experimental Poetry,\n",
       " Topic 46: Formatting Issues,\n",
       " Topic 47: Language Learning Resources,\n",
       " Topic 48: Book genres,\n",
       " Topic 49: Home Improvement,\n",
       " Topic 50: Religious Texts.]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tm.topic_lis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Sep  6 14:51:59 2023 Building and compiling search function\n"
     ]
    }
   ],
   "source": [
    "# load the model if available\n",
    "import pickle\n",
    "with open(\"../Data/SavedTopicRepresentations/TopicGPT_amazonReviews.pkl\", \"rb\") as f:\n",
    "    tm = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what topic 2 is about"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The common topic of the given words is \"Movie Reviews\".\n",
      "\n",
      "Aspects:\n",
      "1. Genre: animated, slasher, noir, zombie, thriller.\n",
      "2. Quality: watchable, unwatchable, dreadful, cheesy, ridiculous.\n",
      "3. Filmmaking: directors, filmmakers, screenwriter, cinematography, filmmaking.\n",
      "4. Audience reaction: scariest, thrilling, hilarious, disappointing, shocking.\n",
      "5. Technical aspects: widescreen, dolby, surround, cinematography, special effects.\n"
     ]
    }
   ],
   "source": [
    "print(tm.topic_lis[2].topic_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT wants to the call the function:  {\n",
      "  \"name\": \"knn_search\",\n",
      "  \"arguments\": \"{\\n  \\\"topic_index\\\": 2,\\n  \\\"query\\\": \\\"Avatar\\\",\\n  \\\"k\\\": 5\\n}\"\n",
      "}\n",
      "Yes, the movie \"Avatar\" is mentioned in topic 2. One of the documents in this topic talks about a version of the movie released on Blu-ray with optimized picture and special features (document index 392). However, it's worth noting that the mention of \"Avatar\" in this document is in comparison to another movie's release on Blu-ray, and the document is not specifically about \"Avatar\" itself.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['Not just because of the 3D, but because this is the version where they made an effort to optimize the picture! They released this on blu ray like like avatar was released. First they release the movie without any optimization and special features. Which means the picture looks better than DVD but not the best that blu ray can be.(which means a grainy looking picture that looks like the characters are in a sandstorm and there is a lack of detail that you expect in a blu ray). The they make the limited edition which is made the way a blu ray is supposed to be down. So if you are wondering which one to choose, this is the one you want! All the features with the visuals to boot!',\n",
       "  'The special effects of this film were wonderful--no debate there--but Armand Assante seems like he\\'s sleep-walking through this movie. It LOOSELY follows the poem at parts and suffers from a severe lack of passion! Compare this film with the 1950\\'s \"Ulysses\", which despite poor effects, had MUCH more passionate and inspired portrayals of the characters by Kirk Douglas and Anthony Quinn among others. Even the classic scene involving Argos the dog was ommitted from this modern creation. Ugh!On the flip side, Athena\\'s portrayal by Isabella Rossellini and the actor playing the god Mercury are very fun to watch in \"The Odyssesy\". Still...not enough to do the story justice.',\n",
       "  \"Homer would have endorsed this rendering of his tale. Full of wonderful special effects, gorgeous Mediterranean vistas, and gods and goddesses who look their parts, this movie is still driven by personalities. The casting was brilliant: Armand Asante is all that one could wish in an Odysseus -- engaging, passionate, mature, heroic in every dimension. Isabella Rosellini and Greta Saachi are perfect in their roles as Athena and Penelope, respectively. Telemachus is a classical adolescent only child, and Antichea, Odysseus's mother, dominates each of her scenes. While this production is eye-candy from beginning to end, the visual appeal cannot distract from the timeless humanity of its characters. There is a reason that Homer's story still reaches us after 3000 years, and this telling does justice to its ancient material.\",\n",
       "  \"The origional creater of The Odyssey, Homer, did a wonderful job producing an action-packed story where you cared about the characters and made you eager to know what would happen next. Thousands of years later, when TVs are in existence, we fail at telling the same story with his exuberance. The motion picture depiction of Homer's work is a lame collection of attempted action scenes that does nothing to keep the watcher entertained throughout this movie. The human characters along with those mythical creatures they encounter respond to events without using common sense that anyone with half a brain would use. Despite all that, assuming the plot and the logic of the characters entertained you enough to keep your eyes open during the movie, you'd realize how all the actors seem undecided on what accent to use or even how to pronounce the names of the others...\",\n",
       "  'I was in the mood for some alien invasion movies the other day and watched a small marathon. As well as such movies as The Arrival, I watched a film called Femalien. Femalien reminds me of a modern remake of The Girl From Starship Venus.Femalien is the story of an alien landing on Earth to study human interactions. The energy being is given the form of a young woman. What follows are scenes of her spying on couples and her interactions with others. The alien is collecting and cataloging experiences for her race who have risen above physicality but still miss it. During her adventures she tries to help a local woman retain ownership of her diner.While the plot is weak I do recommend this film for those times when a movie like Invasion of the Star Creatures or Galaxina is the sort of thing you are looking for.'],\n",
       " [1498, 1346, 1371, 1374, 392])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tm.pprompt(\"Is the movie Avatar mentioned in topic 2?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check the output, we actually inspect the respective document at index 1498: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not just because of the 3D, but because this is the version where they made an effort to optimize the picture! They released this on blu ray like like avatar was released. First they release the movie without any optimization and special features. Which means the picture looks better than DVD but not the best that blu ray can be.(which means a grainy looking picture that looks like the characters are in a sandstorm and there is a lack of detail that you expect in a blu ray). The they make the limited edition which is made the way a blu ray is supposed to be down. So if you are wondering which one to choose, this is the one you want! All the features with the visuals to boot!\n"
     ]
    }
   ],
   "source": [
    "print(tm.topic_lis[2].documents[1498])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us go own with the analysis. Since it is easy to loose the overview over all the topics, lets find out which one is about books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT wants to the call the function:  {\n",
      "  \"name\": \"identify_topic_idx\",\n",
      "  \"arguments\": \"{\\n  \\\"query\\\": \\\"books\\\"\\n}\"\n",
      "}\n",
      "Topic 5 is about books.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tm.pprompt(\"Which topic is about books?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The common topic of the given words is \"Book Reviews\". \n",
      "\n",
      "Various aspects and sub-topics of this topic include:\n",
      "1. Characters: likable, endearing, mighty, crew, pals\n",
      "2. Storyline: satirical, mythological, strange, satirical, endings\n",
      "3. Writing style: well-crafted, inviting, brilliantly, sarcastic\n",
      "4. Themes: philosophical, allusions, belief, religion, obsession\n",
      "5. Critique: uneven, dissatisfaction, novice, pale, unsuccessful\n"
     ]
    }
   ],
   "source": [
    "print(tm.topic_lis[5].topic_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT wants to the call the function:  {\n",
      "  \"name\": \"knn_search\",\n",
      "  \"arguments\": \"{\\n  \\\"topic_index\\\": 5,\\n  \\\"query\\\": \\\"Harry Potter\\\"\\n}\"\n",
      "}\n",
      "Yes, Harry Potter is mentioned in topic 5. The documents in topic 5 that mention Harry Potter are:\n",
      "\n",
      "- Document 23: \"This was the second book I read by Terry. Since then, I've loved Discworld books about Ankh-Morpork (and especially the night watch). To get the full effect, read Feet of Clay, then Jingo right after this one.\"\n",
      "\n",
      "- Document 22: \"Terry Pratchett continues the Discworld tradition in fine fashion with Small Gods. Brutha, a lowly, simple-minded novitiate of a mighty religious empire, has just met his god face-to-face. Actually, heel-to-face, because the mighty god Om is really a not-so-mighty tortoise who is out to reclaim his empire -- with Brutha's help\"\n",
      "\n",
      "- Document 5: \"still well worth reading, whether you are looking for something light to take the edge off a long day at work or something a little heavier to pose a few interesting questions. Small Gods is the only Discworld novel I've read, so I can't compare it to the rest of Pratchett's work, but it grabbed my attention and held it. It's a many-layered book, but Pratchett doesn't force the reader to go any deeper than they feel like. Knowing Latin is an asset in reading (he can come out and say more adult things which he leaves implied in English.) Also, this book doesn't have either a \"British\" or an \"American\" feel, which so much fantasy falls into. Overall, excellent.\"\n",
      "\n",
      "Please note that while Harry Potter is mentioned in these documents, it is not the primary focus of the topic.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['This is another book in the Discworld series, and the second book in the Sam Vimes/Watch story-line. It introduces a couple of characters that become regulars. Well-written and a joy to read.',\n",
       "  \"A well-crafted, sharp-witted tale that will be especially appealing to anyone who has ever read Tolkien and his ilk. Discworld is at once absurd and complex, yet held against the fabric of our reality, it's just as plausible. I'm hard pressed to think of a more likable character than Corporal Carrot. The modest anti-gun message was subtle enough to remain unobtrusive.\",\n",
       "  \"A well-crafted, sharp-witted tale that will be especially appealing to anyone who has ever read Tolkien and his ilk. Discworld is at once absurd and complex, yet held against the fabric of our reality, it's just as plausible. I'm hard pressed to think of a more likable character than Corporal Carrot. The modest anti-gun message was subtle enough to remain unobtrusive.\",\n",
       "  \"Nobody can discuss serious philosophical issues in hysterically funny ways like Terry Pratchett. Small gods is as intelligent exploration of theology and people's relationship to religion as you'll find but if that's not what you want to take from it you'll have a great time laughing through the book. The Turtle Moves.\",\n",
       "  'still well worth reading, whether you are looking for something light to take the edge off a long day at work or something a little heavier to pose a few interesting questions. Small Gods is the only Discworld novel I\\'ve read, so I can\\'t compare it to the rest of Pratchett\\'s work, but it grabbed my attention and held it. It\\'s a many-layered book, but Pratchett doesn\\'t force the reader to go any deeper than they feel like. Knowing Latin is an asset in reading (he can come out and say more adult things which he leaves implied in English.) Also, this book doesn\\'t have either a \"British\" or an \"American\" feel, which so much fantasy falls into. Overall, excellent.',\n",
       "  \"I have read most of this series with great delight. However, Pratchett finally let me down.It plods along, in no particular hurry to let the reader move on to better things.The humor -- what little there is of it -- is forced. It has the feel of sophomore philosophy majors sitting 'round the Quad on a Saturday night, dateless and full of mockery. It is kind of sad.Far from one of his best. PYRAMIDS, any of the series having to do with the Watch or the wondrous three witches -- you will enjoy those far more.This one droops as limply as an Ayn Rand novel.\",\n",
       "  'This was one of the best books in the discworld series i ever read. The Great God Om, wanting to stir up the belief of his followers, decides to manifest himself. He has taken the form of very majestic creatures in the past but for some reason that even he cannot understand he has now manifested himself as a two pound tortise and cannot even do proper miricles anymore. Finding himself with only one true believer (the others trusting in the Quisition)he must try to find a way, using this beliver as his prophet, to regain proper godship. This will be no easy task because the only person that believes in him is not even a preist and tends a small vegetable garden...',\n",
       "  'Mr. Pratchett is a very good writer, and while not every discworld novel is a knock out, none have been bad. Of all of his work this is my favorite. I reread it at least every couple of months. Bruthra and Om [pointy horns] spin a tale quite entertaining that keeps me coming back. I only wish Mr. Pratchett had spent more time talking about the librarian/watcher monks. I hope you enjoy this book as much as I do.',\n",
       "  \"I've read quite a few of the Pratchett books, mostly at the insistence of my 13 year old son who loves these novels. This ranks right up there among my favorites, and is intriguing for the allusions to so many of the world's religions. The author is clearly well read, and catching these little references produced many aha! moments, I confess that I even lugged home a compendium on world religion to dig out the paralells and little obscure references - and there is alot more to this book than meets the eye. Well, but in the end, I find myself in complete sympathy with the conclusion, and thus a truly satisfying narrative from start to finish. I should add that this approach to reading this book drove my son absolutely crazy, and in fact taken only at face value, no references or allusions the book is still a great read.\",\n",
       "  'A really fun book, well worth your time if you like Pratchett. None of our old pals [Rincewind, Gaspode, Vimes, Susan] put in an appearence [other than the guy WHO TALKS LIKE THIS and an old monk with a broom, and they but briefly], but it does add some info to the overall Discworld mythology, and provides some laughs as well.',\n",
       "  'although this book was an intentional attack on organized religion of the past, i thought that it went to far in trying to humor the audience. In light of this book being fiction, Pratchett still wanders a little far into even joking about something that is as important to millions of people as a supreme power.even so, the comedy good throughout. the characters were a little shallow at times, and my own obsession with imagery in fantasy novels (see Guy Gavriel Kay), made me feeling a bit disappointed in such a huge phenomenon as Terry Pratchett.',\n",
       "  \"This was the first Terry Pratchett novel I read. Now I have 10 others. This book was funny in so many ways. One of my favorite things is Mr. Pratchett's use of footnotes. The one about Bloody Stupid Johnson is the best! I highly recommend the whole Discworld series to every reader.\",\n",
       "  'Like all other Pratchett novels, this one is extremely witty, sarcastic, and equally intelligent. Let\\'s start a new religion- we\\'ll call it \"Pratchettism\"',\n",
       "  \"This book features one of the first Troll-Dwarf Friendships ever to hit the stands. Cuddy the dwarf does a great job of killing the practice dummy explaning that the fugative does not have to be able to answer questions. As well as the return of the 6 foot(adopted) dwarf, Carrot, the librairan, who is a very funny ape(don't call him a monkey), and Gaspode the wonder dog. He is very good at persuasion.\",\n",
       "  \"My sister read this book aloud to me on a road trip from Seattle to California. It is a work of multilayered artistry to be appreciated from the spiritual to the fantastic. It's also fun and unlike most of Pratchett's books you actually care about most of the characters. Reads well out loud as well. Like many readers here, I see it as the author's pinnacle, the book that he will be remembered for. Here's to immortality, Mr. Pratchett!\",\n",
       "  \"I think this may be McCall Smith's funniest work, and strangely his most endearing. His affection for his characters - three very thin, very tall, very overeducated linguists - is obvious in every paragraph. The humor that ensues from Professor Doctor Doctor von Igelfeld's adventures and misadventures is honest and endearing, while gently reminding us not to take ourselves quite so seriously.\",\n",
       "  \"This was the second book I read by Terry. Since then, I've loved Discworld books about Ankh-Morpork (and especially the night watch). To get the full effect, read Feet of Clay, then Jingo right after this one.\",\n",
       "  \"Terry Pratchett continues the Discworld tradition in fine fashion with Small Gods. Brutha, a lowly, simple-minded novitiate of a mighty religious empire, has just met his god face-to-face. Actually, heel-to-face, because the mighty god Om is really a not-so-mighty tortoise who is out to reclaim his empire -- with Brutha's help\",\n",
       "  'Meet Carrot, Angua, Vimes and the other watchmen in their debut performance as they leap into the Discworld. This is the first of the \"Vimes\" stories, so it is important for a basic knowledge of people you`ll be seeing a lot more of(depending on the humour of old man Pratchett). It`s no less funny than the others, while at the same time introducing detective work into the series as Vimes and his crew bumble along on a sinister case. They are, after all, policemen(no offense to policemen in general). Read it and love it. It`s different, but at least as good as the other Rincewind and Death books.',\n",
       "  'While the ideas that lay the framework for this book are solid, everything else is lacking.The humor is forced, awkward, and on occasion plagarized. A nearly verbatim quote from a Monty Python sketch stands out in my mind.The plot is slow and seldom rivetting. The only thing that kept me turning the pages at some points was my search for the origin of the \"REMIND ME AGAIN HOW THE HORSE-SHAPED ONES MOVE\" quote.The character development is poor. The main character, Brutha, makes a sudden transition from a bumbling idiot to a sagely leader-of-men and bargainer-with-gods. I wasn\\'t convinced.The ideas, however, are the salvation of this book. Read it if you have a good imagination...you\\'ll need it to make up a new story around the concept Pratchett so poorly utilizes.'],\n",
       " [23, 22, 5, 8, 10, 16, 14, 17, 12, 2, 24, 21, 30, 0, 11, 28, 1, 7, 25, 26])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tm.pprompt(\"Is Harry Potter mentioned in topic 5?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic 0 (\"Musical genres and characteristics\") sounds a bit general and from the visual inspection it seems to contain a lot of documents. So let's break it down a little bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT wants to the call the function:  {\n",
      "  \"name\": \"split_topic_kmeans\",\n",
      "  \"arguments\": \"{\\n  \\\"topic_idx\\\": 0,\\n  \\\"inplace\\\": true\\n}\"\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs completed: 100%| ██████████ 100/100 [00:01]\n",
      "Computing word-topic matrix: 100%|██████████| 1/1 [00:00<00:00,  1.44it/s]\n",
      "Epochs completed: 100%| ██████████ 100/100 [00:03]\n",
      "Epochs completed: 100%| ██████████ 100/100 [00:02]\n",
      "100%|██████████| 1/1 [00:09<00:00,  9.31s/it]\n",
      "Epochs completed: 100%| ██████████ 100/100 [00:01]\n",
      "Computing word-topic matrix: 100%|██████████| 1/1 [00:00<00:00,  4.57it/s]\n",
      "Epochs completed: 100%| ██████████ 100/100 [00:01]\n",
      "Epochs completed: 100%| ██████████ 100/100 [00:01]\n",
      "100%|██████████| 1/1 [00:10<00:00, 10.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have split topic 0 into two subtopics. \n",
      "\n",
      "Subtopic 50: Music Genres\n",
      "The common theme of the given words is \"Music genres and styles\". Various aspects and subtopics of this topic include genres such as ballads, funk, hip-hop, rock, ambient, reggae, disco; instruments such as acoustic, drums, riffs, violin, saxophone, flute, harmonica; styles like acoustic, funky, mellow, jazzy, melodic, eclectic; performers such as rappers, musicians, composers, violinists, duets; and musical elements such as beats, rhythms, groove, solos, and harmonies.\n",
      "\n",
      "Subtopic 51: Music Remasters and Performances\n",
      "The common theme of the given words is \"Music\". Aspects and subtopics of this topic include musical genres such as musicals, jazz, rock, pop, soul; instruments like orchestra, saxophone, drums, flute, keyboard; performers such as concerts, performers, vocalists, drummers, Clapton; sound quality aspects such as remastered, surround, mastering, mastered, acoustic; and critiques that include disappointing, inadequate, inferior, horrid, and atrocious.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Topic 0: Sci-fi TV show.,\n",
       " Topic 1: Film Genres,\n",
       " Topic 2: Paranormal phenomena and UFO sightings,\n",
       " Topic 3: Earbuds and Headsets,\n",
       " Topic 4: Book Review Topics,\n",
       " Topic 5: Gluten-free Cookbook,\n",
       " Topic 6: Air Mattresses,\n",
       " Topic 7: Crime and Investigation.,\n",
       " Topic 8: Printer Troubleshooting,\n",
       " Topic 9: Hiking Footwear,\n",
       " Topic 10: Shapewear,\n",
       " Topic 11: Dance Instruction,\n",
       " Topic 12: Parenting and Education,\n",
       " Topic 13: Electronic Gadgets,\n",
       " Topic 14: Video Games,\n",
       " Topic 15: MP3 Player Issues,\n",
       " Topic 16: Camera Accessories,\n",
       " Topic 17: Power Adapters,\n",
       " Topic 18: Product Quality,\n",
       " Topic 19: Ancient civilizations and anthropology.,\n",
       " Topic 20: Router Connectivity,\n",
       " Topic 21: Technical Issues,\n",
       " Topic 22: Puritanical Society,\n",
       " Topic 23: Sci-fi Space Exploration,\n",
       " Topic 24: Beauty Products,\n",
       " Topic 25: Sexual Vibrators,\n",
       " Topic 26: Home Safety,\n",
       " Topic 27: Product Quality,\n",
       " Topic 28: Customer Service Experience,\n",
       " Topic 29: Textbook Quality,\n",
       " Topic 30: Programming Documentation,\n",
       " Topic 31: Hardware Tools,\n",
       " Topic 32: Product Quality,\n",
       " Topic 33: Educational Toys,\n",
       " Topic 34: Appliances,\n",
       " Topic 35: Kitchenware,\n",
       " Topic 36: Supernatural Witches,\n",
       " Topic 37: Horror Comics,\n",
       " Topic 38: Dystopian society,\n",
       " Topic 39: Emotional Turmoil,\n",
       " Topic 40: Book genres,\n",
       " Topic 41: Economic and Political Critique,\n",
       " Topic 42: Poorly Written Erotica,\n",
       " Topic 43: Dystopian Surveillance State,\n",
       " Topic 44: Experimental Poetry,\n",
       " Topic 45: Formatting Issues,\n",
       " Topic 46: Language Learning Resources,\n",
       " Topic 47: Book genres,\n",
       " Topic 48: Home Improvement,\n",
       " Topic 49: Religious Texts.,\n",
       " Topic 50: Music Genres,\n",
       " Topic 51: Music Remasters and Performances]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tm.pprompt(\"please split topic 0 into subtopics. Do this inplace\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the topic 0 was split into two topics. One on music genres and the other one on remastering of music. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "On the other hand, topic 0 and topic 2 seem very similar. Let's find out more about the difference between the two: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT wants to the call the function:  {\n",
      "  \"name\": \"get_topic_information\",\n",
      "  \"arguments\": \"{\\n  \\\"topic_idx_lis\\\": [0, 2]\\n}\"\n",
      "}\n",
      "Topic 0 is about sci-fi TV shows, while topic 2 is about paranormal phenomena and UFO sightings. \n",
      "\n",
      "The similarities between the two topics seem to be quite limited based on the available descriptions. Both topics involve speculative elements and may touch on the realm of the unknown. However, the specific sub-topics and aspects mentioned in the descriptions of the topics do not share much overlap.\n",
      "\n",
      "Topic 0, the sci-fi TV show topic, includes sub-topics such as plot and story, setting and characters, originality and creativity, quality and highlights, and viewer experience and opinions. The top words associated with this topic include terms like \"finale\", \"timeline\", \"rewind\", \"hints\", \"galaxy\", \"starship\", \"crew\", and \"originality\".\n",
      "\n",
      "Topic 2, the paranormal and UFO sightings topic, includes sub-topics and aspects such as UFO sightings, unexplained events, belief and speculation, controversy and debunking, and involvement and research. The top words associated with this topic include terms like \"sightings\", \"paranormal\", \"phenomena\", \"UFO\", \"encounters\", \"unexplained\", \"speculation\", and \"believer\".\n",
      "\n",
      "In summary, while both topics may involve elements of speculation and the unknown, they seem to focus on different subject matter – sci-fi TV shows for topic 0 and paranormal phenomena and UFO sightings for topic 2.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: '\\n            Topic index: 0\\n            Topic name: Sci-fi TV show.\\n            Topic description: The common topic of the given words is \"TV shows\" or \"television series\". \\n\\nSub-topics and aspects of this topic include:\\n1. Plot and story: \"finale\", \"timeline\", \"rewind\", \"rehash\", \"hints\"\\n2. Setting and characters: \"galaxy\", \"starship\", \"crew\", \"diehard\"\\n3. Originality and creativity: \"originality\", \"spawned\", \"flawless\"\\n4. Quality and highlights: \"highlights\", \"vice\", \"pacing\", \"woefully\", \"kinks\"\\n5. Viewer experience and opinions: \"farewell\", \"so-so\", \"unintentionally\", \"desperate\", \"on-going\"\\n            Topic topwords: [\\'finale\\', \\'timeline\\', \\'gate\\', \\'rewind\\', \\'rehash\\', \\'hints\\', \\'galaxy\\', \\'served\\', \\'continuation\\', \\'crew\\', \\'starship\\', \\'diehard\\', \\'originality\\', \\'aired\\', \\'orginal\\', \\'spawned\\', \\'flawless\\', \\'highlights\\', \\'vice\\', \\'pacing\\', \\'woefully\\', \\'kinks\\', \\'farewell\\', \\'so-so\\', \\'unintentionally\\', \\'desperate\\', \\'on-going\\', \\'watcher\\', \\'verry\\', \\'crimes\\', \\'arctic\\', \\'wicked\\', \\'stance\\', \\'swearing\\', \\'remaining\\', \\'namely\\', \\'settled\\', \\'tackle\\', \\'travelling\\', \\'babble\\', \\'trial\\', \\'appearances\\', \\'self-absorbed\\', \\'tricks\\', \\'peek\\', \\'eager\\', \\'guard\\', \\'gould\\', \\'span\\', \\'dedicated\\', \\'tying\\', \\'saga\\', \\'intentionally\\', \\'convoluted\\', \\'zing\\', \\'truely\\', \\'devoid\\', \\'dependent\\', \\'infinite\\', \\'parade\\', \\'loyal\\', \\'altered\\', \\'poignant\\', \\'parallel\\', \\'branch\\', \\'profoundly\\', \\'committed\\', \\'resolve\\', \\'gaps\\', \\'trek\\', \\'judgement\\', \\'philosophical\\', \\'entry\\', \\'souls\\', \\'sci\\', \\'versa\\', \\'crammed\\', \\'rocked\\', \\'integrated\\', \\'revisit\\', \\'manipulate\\', \\'owning\\', \\'comeback\\', \\'succesful\\', \\'regulations\\', \\'purposes\\', \\'assuming\\', \\'preparing\\', \\'welcome\\', \\'formed\\', \\'fundamental\\', \\'secured\\', \\'concentrated\\', \\'o.k\\', \\'dinner\\', \\'arriving\\', \\'pleasent\\', \\'prefered\\', \\'satisfaction\\', \\'wrap\\', \\'snacks\\', \\'closure\\', \\'potty\\', \\'installment\\', \\'rainy\\', \\'fluff\\', \\'slipped\\', \\'interplay\\', \\'compression\\', \\'prayer\\', \\'pre-historical\\', \\'positions\\', \\'pre\\', \\'positioned\\', \\'positively\\', \\'praying\\', \\'poses\\', \\'precise\\', \\'ports\\', \\'pose\\', \\'portrays\\', \\'portraying\\', \\'preach\\', \\'preacher\\', \\'portrayals\\', \\'portray\\', \\'preaches\\', \\'preaching\\', \\'preachy\\', \\'preceding\\', \\'portrait\\', \\'precious\\', \\'pre-pregnancy\\', \\'possess\\', \\'post-apocalyptic\\', \\'pray\\', \\'pots\\', \\'practiced\\', \\'potter\\', \\'ppl\\', \\'powershot\\', \\'pound\\', \\'powers\\', \\'pounding\\', \\'pounds\\', \\'powerfully\\', \\'powered\\', \\'powder\\', \\'pour\\', \\'poured\\', \\'poverty\\', \\'potentially\\', \\'potente\\', \\'posture\\', \\'posts\\', \\'praising\\', \\'possibilities\\', \\'possibility\\', \\'pow\\', \\'postage\\', \\'postcard\\', \\'praised\\', \\'possesses\\', \\'prague\\', \\'posted\\', \\'poster\\', \\'posters\\', \\'practicing\\', \\'postings\\', \\'practices\\', \\'postpartum\\', \\'praetorians\\', \\'precisely\\', \\'a/c\\', \\'predator\\', \\'profile\\', \\'proficient\\', \\'professors\\', \\'professionals\\', \\'professionally\\', \\'prof.\\', \\'producto\\', \\'productive\\', \\'productions\\', \\'product.the\\', \\'producing\\', \\'produces\\', \\'producer\\', \\'prodigal\\', \\'proclaimed\\', \\'processes\\', \\'procedures\\', \\'procedure\\', \\'problematic\\', \\'probe\\', \\'prob\\', \\'privilege\\', \\'privacy\\', \\'profit\\', \\'profits\\', \\'programmed\\', \\'programmer\\', \\'propaganda\\', \\'prop\\', \\'proofread\\', \\'pronunciation\\', \\'pronounciation\\', \\'pronounce\\', \\'prongs\\', \\'prone\\', \\'prompted\\', \\'prompt\\', \\'promotion\\', \\'pristine\\', \\'promoting\\', \\'promote\\', \\'promo\\', \\'promising\\', \\'promises\\', \\'prominent\\', \\'prologue\\', \\'proles\\', \\'progressive\\', \\'progresses\\', \\'progressed\\', \\'programmers\\', \\'promotes\\', \\'prisoners\\', \\'prison\\', \\'priority\\', \\'presenting\\', \\'presence\\', \\'prescription\\', \\'prescient\\', \\'preparation\\', \\'prep\\', \\'portions\\', \\'premium\\', \\'premiere\\', \\'prelude\\', \\'prejudice\\', \\'preserve\\', \\'pregnancy\\', \\'preference\\', \\'preface\\', \\'predicts\\', \\'predictions\\', \\'prediction\\', \\'predicting\\', \\'predicted\\', \\'predictability\\', \\'predict\\', \\'predecessors\\', \\'predators\\', \\'prefers\\', \\'precision\\', \\'preserved\\', \\'president\\', \\'principles\\', \\'principle\\', \\'principal\\', \\'princess\\', \\'prince\\', \\'primer\\', \\'primal\\', \\'pride\\', \\'pricey\\', \\'priceless\\', \\'prey\\', \\'preset\\', \\'previews\\', \\'preview\\', \\'prevents\\', \\'prevalent\\', \\'pretension\\', \\'pretends\\', \\'pretend\\', \\'presto\\', \\'prestigious\\', \\'pressured\\', \\'pressing\\', \\'pressed\\', \\'previewed\\', \\'portfolio\\', \\'polidoro\\', \\'port\\', \\'ph.d.\\', \\'peyton\\', \\'pets\\', \\'petite\\', \\'peterson\\', \\'peters\\', \\'pete\\', \\'pesky\\', \\'perverse\\', \\'pervasive\\', \\'persuasion\\', \\'phantom\\', \\'perspectives\\', \\'personaly\\', \\'personalities\\', \\'persona\\', \\'perry\\', \\'perpetual\\', \\'perpetrated\\', \\'pero\\', \\'permanently\\', \\'permanent\\', \\'perks\\', \\'perkins\\', \\'persons\\', \\'periods\\', \\'phase\\', \\'phd\\', \\'pianist\\', \\'piaa\\', \\'physics\\', \\'physically\\', \\'phrasing\\', \\'phrasebook\\', \\'phrase\\', \\'photosmart\\', \\'photographs\\', \\'photographer\\', \\'photographed\\', \\'phases\\', \\'phony\\', \\'phoned\\', \\'phoenix\\', \\'philosophies\\', \\'philosopher\\', \\'philly\\', \\'phillips\\', \\'philadelphia\\', \\'phil\\', \\'phenomenon\\', \\'phenomenal\\', \\'phenomena\\', \\'phones\\', \\'performs\\', \\'performers\\', \\'performer\\', \\'peak\\', \\'peaceful\\', \\'pays\\', \\'payed\\', \\'pause\\', \\'patti\\', \\'pattern\\', \\'patrol\\', \\'patinkin\\', \\'patients\\', \\'patient\\', \\'peaks\\', \\'paths\\', \\'patches\\', \\'patched\\', \\'patch\\', \\'pat\\', \\'pastor\\', \\'paste\\', \\'password\\', \\'passionately\\', \\'passionate\\', \\'passes\\', \\'passage\\', \\'pathos\\', \\'peanuts\\', \\'peck\\', \\'pedal\\', \\'perfection\\', \\'percussion\\', \\'perceptions\\', \\'perception\\', \\'percent\\', \\'perabo\\', \\'peppered\\', \\'pentium\\', \\'pens\\', \\'pennsylvania\\', \\'penguin\\', \\'penetrating\\', \\'penelope\\', \\'pencil\\', \\'penchant\\', \\'pen\\', \\'pellegrino\\', \\'peice\\', \\'pegs\\', \\'peggy\\', \\'peers\\', \\'peeling\\', \\'peel\\', \\'pedestrian\\', \\'pedantic\\', \\'pic\\', \\'portability\\', \\'pickup\\', \\'pics\\', \\'poison\\', \\'pointlessly\\', \\'pointing\\', \\'pointed\\', \\'poets\\', \\'poetic\\', \\'poet\\', \\'poems\\', \\'poem\\', \\'poe\\', \\'podcast\\', \\'poisoning\\', \\'pockets\\', \\'plugs\\', \\'plugging\\', \\'plug-in\\', \\'plotting\\', \\'plotted\\', \\'plotline\\', \\'plotless\\', \\'plot.it\\', \\'plodding\\', \\'plight\\', \\'pliers\\', \\'pny\\', \\'plethora\\', \\'poking\\', \\'polemic\\', \\'porn\\', \\'por\\', \\'population\\', \\'popularity\\', \\'populace\\', \\'pops\\', \\'popping\\', \\'popcorn\\', \\'pop-up\\', \\'poorest\\', \\'poop\\', \\'pole\\', \\'pool\\', \\'ponderous\\', \\'pondering\\', \\'ponder\\', \\'pompous\\', \\'politicians\\', \\'politically\\', \\'polishes\\', \\'polish\\', \\'properties\\', \\'policies\\', \\'policeman\\', \\'pooch\\', \\'plentiful\\', \\'pledge\\', \\'pleasures\\', \\'pitched\\', \\'pitch\\', \\'pita\\', \\'pit\\', \\'pistols\\', \\'pissed\\', \\'piper\\', \\'pipe\\', \\'pioneers\\', \\'pioneer\\', \\'pins\\', \\'pitchshifter\\', \\'pinnacle\\', \\'pinch\\', \\'pilots\\', \\'pilot\\', \\'pills\\', \\'pillows\\', \\'pill\\', \\'pilgrimage\\', \\'pig\\', \\'pierre\\', \\'pieced\\', \\'pie\\', \\'pine\\', \\'pitfalls\\', \\'pitiful\\', \\'pivotal\\', \\'pleasurable\\', \\'pleasing\\', \\'pleasantly\\', \\'playstation\\', \\'playset\\', \\'playlists\\', \\'playlist\\', \\'playboy\\', \\'playback\\', \\'playable\\', \\'plausible\\', \\'platform\\', \\'plates\\', \\'plate\\', \\'plants\\', \\'plant\\', \\'plans\\']',\n",
       " 2: '\\n            Topic index: 2\\n            Topic name: Paranormal phenomena and UFO sightings\\n            Topic description: The common topic of the given words is \"Paranormal Phenomena\". \\n\\nAspects and sub-topics of the topic include:\\n1. UFO Sightings: Encounters, Witnesses, Reported, Sightings\\n2. Unexplained Events: Unexplained, Anecdotes, Incident, Occurrences\\n3. Belief and Speculation: Believer, Speculation, Believed, Possibilities\\n4. Controversy and Debunking: Controversial, Debunked, Dubious, Tendency\\n5. Involvement and Research: Involvement, Researching, Observations, Findings\\n            Topic topwords: [\\'sightings\\', \\'paranormal\\', \\'phenomena\\', \\'ufo\\', \\'encounters\\', \\'unexplained\\', \\'b/c\\', \\'speculation\\', \\'believer\\', \\'relates\\', \\'anecdotes\\', \\'copyright\\', \\'sorely\\', \\'disjointed\\', \\'buff\\', \\'floating\\', \\'visited\\', \\'witnesses\\', \\'stormy\\', \\'admits\\', \\'bin\\', \\'rarity\\', \\'reported\\', \\'ties\\', \\'sighting\\', \\'entitled\\', \\'throws\\', \\'memoir\\', \\'wins\\', \\'stance\\', \\'letdown\\', \\'journalist\\', \\'surrounding\\', \\'hunting\\', \\'incident\\', \\'scenario\\', \\'credibility\\', \\'preview\\', \\'happenings\\', \\'admission\\', \\'reports\\', \\'perpetrated\\', \\'nut\\', \\'delve\\', \\'aunt\\', \\'rampant\\', \\'believed\\', \\'capable\\', \\'touches\\', \\'therein\\', \\'insulting\\', \\'lunatics\\', \\'located\\', \\'pedestrian\\', \\'glance\\', \\'controversial\\', \\'participants\\', \\'conspiracy\\', \\'notably\\', \\'trait\\', \\'possibilities\\', \\'loosely\\', \\'observations\\', \\'involvement\\', \\'hypothesis\\', \\'dubious\\', \\'findings\\', \\'tendency\\', \\'debunked\\', \\'accounts\\', \\'affect\\', \\'positions\\', \\'skeptical\\', \\'pushes\\', \\'mildly\\', \\'sap\\', \\'obsession\\', \\'reporting\\', \\'woo\\', \\'engage\\', \\'assure\\', \\'informs\\', \\'distracted\\', \\'goddesses\\', \\'activity\\', \\'sincere\\', \\'underlying\\', \\'researching\\', \\'applied\\', \\'forgetting\\', \\'behaving\\', \\'theoretical\\', \\'origins\\', \\'occurrences\\', \\'origin\\', \\'plague\\', \\'parallel\\', \\'questionable\\', \\'catagory\\', \\'presenting\\', \\'exhausting\\', \\'generate\\', \\'appearing\\', \\'ramblings\\', \\'mentions\\', \\'documents\\', \\'intrigued\\', \\'unfamiliar\\', \\'approached\\', \\'writings\\', \\'disorganized\\', \\'rang\\', \\'logical\\', \\'concentrated\\', \\'incoherent\\', \\'dross\\', \\'examine\\', \\'understandable\\', \\'thesis\\', \\'adjective\\', \\'teller\\', \\'lengthy\\', \\'blended\\', \\'warns\\', \\'skewed\\', \\'answered\\', \\'reprint\\', \\'scraps\\', \\'storytelling\\', \\'mixture\\', \\'mighty\\', \\'wacky\\', \\'ounce\\', \\'screenwriter\\', \\'goofy\\', \\'wave\\', \\'offbeat\\', \\'phenomenal\\', \\'extraordinary\\', \\'chasing\\', \\'staring\\', \\'aliens\\', \\'dates\\', \\'shocker\\', \\'valid\\', \\'satisfy\\', \\'vehicles\\', \\'ghosts\\', \\'eyed\\', \\'spooky\\', \\'shaped\\', \\'bridge\\', \\'grain\\', \\'gospel\\', \\'composed\\', \\'dose\\', \\'mixes\\', \\'shrink\\', \\'walked\\', \\'ceiling\\', \\'flashlight\\', \\'telephone\\', \\'proles\\', \\'prague\\', \\'programmers\\', \\'possess\\', \\'preach\\', \\'praised\\', \\'praising\\', \\'pray\\', \\'promoting\\', \\'prayer\\', \\'programmer\\', \\'post-apocalyptic\\', \\'praying\\', \\'promotion\\', \\'progresses\\', \\'prompt\\', \\'praetorians\\', \\'postcard\\', \\'pre\\', \\'pre-historical\\', \\'possesses\\', \\'progressed\\', \\'postage\\', \\'prompted\\', \\'prone\\', \\'pre-pregnancy\\', \\'possibility\\', \\'progressive\\', \\'prominent\\', \\'practicing\\', \\'postpartum\\', \\'posts\\', \\'pounds\\', \\'pounding\\', \\'promote\\', \\'pound\\', \\'pour\\', \\'promo\\', \\'posture\\', \\'potty\\', \\'potente\\', \\'potter\\', \\'pots\\', \\'potentially\\', \\'promising\\', \\'prologue\\', \\'postings\\', \\'poverty\\', \\'practices\\', \\'posted\\', \\'practiced\\', \\'ppl\\', \\'powershot\\', \\'powers\\', \\'poured\\', \\'promotes\\', \\'poster\\', \\'promises\\', \\'posters\\', \\'powered\\', \\'powder\\', \\'pow\\', \\'powerfully\\', \\'prisoners\\', \\'profile\\', \\'programmed\\', \\'pretension\\', \\'pretends\\', \\'procedures\\', \\'pretend\\', \\'presto\\', \\'prestigious\\', \\'pressured\\', \\'pressing\\', \\'processes\\', \\'pressed\\', \\'president\\', \\'proclaimed\\', \\'preset\\', \\'preserved\\', \\'prodigal\\', \\'producer\\', \\'preserve\\', \\'produces\\', \\'presence\\', \\'prevalent\\', \\'prescription\\', \\'prevents\\', \\'problematic\\', \\'priority\\', \\'pristine\\', \\'principles\\', \\'principle\\', \\'privacy\\', \\'privilege\\', \\'principal\\', \\'princess\\', \\'prince\\', \\'primer\\', \\'primal\\', \\'pride\\', \\'pricey\\', \\'priceless\\', \\'prob\\', \\'prey\\', \\'previews\\', \\'previewed\\', \\'probe\\', \\'procedure\\', \\'preacher\\', \\'prescient\\', \\'producing\\', \\'predicted\\', \\'predictability\\', \\'predict\\', \\'proficient\\', \\'predecessors\\', \\'predators\\', \\'predator\\', \\'precision\\', \\'precisely\\', \\'precise\\', \\'prison\\', \\'profit\\', \\'profits\\', \\'precious\\', \\'preceding\\', \\'profoundly\\', \\'preachy\\', \\'preaching\\', \\'preaches\\', \\'professors\\', \\'preparing\\', \\'predicting\\', \\'professionals\\', \\'preparation\\', \\'prep\\', \\'premium\\', \\'premiere\\', \\'prelude\\', \\'prejudice\\', \\'pregnancy\\', \\'prefers\\', \\'product.the\\', \\'preference\\', \\'prefered\\', \\'productions\\', \\'productive\\', \\'producto\\', \\'preface\\', \\'predicts\\', \\'predictions\\', \\'prof.\\', \\'professionally\\', \\'prediction\\', \\'a/c\\', \\'pompous\\', \\'positioned\\', \\'philosophies\\', \\'philosophical\\', \\'philosopher\\', \\'philly\\', \\'phillips\\', \\'philadelphia\\', \\'phil\\', \\'phenomenon\\', \\'phd\\', \\'phases\\', \\'phase\\', \\'phoenix\\', \\'phantom\\', \\'peyton\\', \\'pets\\', \\'petite\\', \\'peterson\\', \\'peters\\', \\'pete\\', \\'pesky\\', \\'perverse\\', \\'pervasive\\', \\'persuasion\\', \\'perspectives\\', \\'ph.d.\\', \\'persons\\', \\'phoned\\', \\'phony\\', \\'pilot\\', \\'pills\\', \\'pillows\\', \\'pill\\', \\'pilgrimage\\', \\'pig\\', \\'pierre\\', \\'pieced\\', \\'pie\\', \\'pics\\', \\'picky\\', \\'phones\\', \\'pickup\\', \\'pianist\\', \\'piaa\\', \\'physics\\', \\'physically\\', \\'phrasing\\', \\'phrasebook\\', \\'phrase\\', \\'photosmart\\', \\'photographs\\', \\'photographer\\', \\'photographed\\', \\'pic\\', \\'pilots\\', \\'personaly\\', \\'persona\\', \\'peice\\', \\'pegs\\', \\'peggy\\', \\'peers\\', \\'peeling\\', \\'peel\\', \\'peek\\', \\'pedantic\\', \\'pedal\\', \\'peck\\', \\'peanuts\\', \\'pellegrino\\', \\'peaks\\', \\'peaceful\\', \\'pays\\', \\'payed\\', \\'pause\\', \\'patti\\', \\'pattern\\', \\'patrol\\', \\'patinkin\\', \\'patients\\', \\'patient\\', \\'paths\\', \\'peak\\', \\'personalities\\', \\'pen\\', \\'pencil\\', \\'perry\\', \\'perpetual\\', \\'pero\\', \\'permanently\\', \\'permanent\\', \\'perks\\', \\'perkins\\', \\'periods\\', \\'performs\\', \\'performers\\', \\'performer\\', \\'penchant\\', \\'perfection\\', \\'perceptions\\', \\'perception\\', \\'percent\\', \\'perabo\\', \\'peppered\\', \\'pentium\\', \\'pens\\', \\'pennsylvania\\', \\'penguin\\', \\'penetrating\\', \\'penelope\\', \\'percussion\\', \\'pinch\\', \\'pine\\', \\'pinnacle\\', \\'prongs\\', \\'politicians\\', \\'politically\\', \\'polishes\\', \\'polish\\', \\'polidoro\\', \\'policies\\', \\'policeman\\', \\'polemic\\', \\'pole\\', \\'poking\\', \\'ponder\\', \\'poisoning\\', \\'pointlessly\\', \\'pointing\\', \\'pointed\\', \\'poignant\\', \\'poets\\', \\'poetic\\', \\'poet\\', \\'poems\\', \\'poem\\', \\'poe\\', \\'podcast\\', \\'poison\\', \\'pockets\\', \\'pondering\\', \\'pooch\\', \\'poses\\', \\'pose\\', \\'ports\\', \\'portrays\\', \\'portraying\\', \\'portrayals\\', \\'portray\\', \\'portrait\\', \\'portions\\', \\'portfolio\\', \\'portability\\', \\'ponderous\\', \\'port\\', \\'por\\', \\'population\\', \\'popularity\\', \\'populace\\', \\'pops\\', \\'popping\\', \\'popcorn\\', \\'pop-up\\', \\'poorest\\', \\'poop\\', \\'pool\\', \\'porn\\', \\'pny\\', \\'plugs\\', \\'plugging\\', \\'plant\\', \\'plans\\', \\'planned\\', \\'planets\\', \\'planes\\', \\'plainly\\', \\'placing\\', \\'placement\\', \\'pixelated\\', \\'pivotal\\', \\'pitiful\\', \\'plants\\']'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tm.pprompt(\"What are the differences and similarities of topic 0 and topic 2?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The topics seem fairly related, so we can merge them into one topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT wants to the call the function:  {\n",
      "  \"name\": \"combine_topics\",\n",
      "  \"arguments\": \"{\\n  \\\"topic_idx_lis\\\": [0, 2],\\n  \\\"inplace\\\": true\\n}\"\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs completed: 100%| ██████████ 100/100 [00:01]\n",
      "Computing word-topic matrix: 100%|██████████| 1/1 [00:02<00:00,  2.39s/it]\n"
     ]
    },
    {
     "ename": "SystemError",
     "evalue": "CPUDispatcher(<function NNDescent._init_search_function.<locals>.search_closure at 0x0000026C0365B600>) returned a result with an exception set",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\arik_\\anaconda3\\envs\\llm_sem\\Lib\\site-packages\\numba\\core\\serialize.py:30\u001b[0m, in \u001b[0;36m_numba_unpickle\u001b[1;34m(address, bytedata, hashed)\u001b[0m\n\u001b[0;32m     27\u001b[0m _unpickled_memo \u001b[39m=\u001b[39m {}\n\u001b[1;32m---> 30\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_numba_unpickle\u001b[39m(address, bytedata, hashed):\n\u001b[0;32m     31\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Used by `numba_unpickle` from _helperlib.c\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \n\u001b[0;32m     33\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[39m        unpickled object\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mSystemError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;31mSystemError\u001b[0m: <function _numba_unpickle at 0x0000026C6F2536A0> returned a result with an exception set",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mSystemError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m tm\u001b[39m.\u001b[39;49mpprompt(\u001b[39m\"\u001b[39;49m\u001b[39mplease merge topic 0 and topic 2. Do this inplace\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\users\\arik_\\documents\\dokumente\\studium\\master\\llm_seminar\\project\\src\\topicgpt\\TopicGPT.py:330\u001b[0m, in \u001b[0;36mTopicGPT.pprompt\u001b[1;34m(self, query, return_function_result)\u001b[0m\n\u001b[0;32m    320\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpprompt\u001b[39m(\u001b[39mself\u001b[39m, query:\u001b[39mstr\u001b[39m, return_function_result: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mobject\u001b[39m:\n\u001b[0;32m    321\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    322\u001b[0m \u001b[39m    This function prompts the model with the query and prints the answer.\u001b[39;00m\n\u001b[0;32m    323\u001b[0m \u001b[39m    params:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[39m        function_result: The result of the function call.    \u001b[39;00m\n\u001b[0;32m    328\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 330\u001b[0m     answer, function_result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprompt(query)\n\u001b[0;32m    332\u001b[0m     \u001b[39mprint\u001b[39m(answer)\n\u001b[0;32m    334\u001b[0m     \u001b[39mif\u001b[39;00m return_function_result:\n",
      "File \u001b[1;32mc:\\users\\arik_\\documents\\dokumente\\studium\\master\\llm_seminar\\project\\src\\topicgpt\\TopicGPT.py:311\u001b[0m, in \u001b[0;36mTopicGPT.prompt\u001b[1;34m(self, query)\u001b[0m\n\u001b[0;32m    301\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprompt\u001b[39m(\u001b[39mself\u001b[39m, query: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m (\u001b[39mstr\u001b[39m, \u001b[39mobject\u001b[39m):\n\u001b[0;32m    302\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    303\u001b[0m \u001b[39m    This function prompts the model with the query. Please Have a look at the TopicPrompting class for more details on available functions for prompting the model.\u001b[39;00m\n\u001b[0;32m    304\u001b[0m \u001b[39m    params:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    308\u001b[0m \u001b[39m        function_result: The result of the function call.\u001b[39;00m\n\u001b[0;32m    309\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 311\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtopic_prompting\u001b[39m.\u001b[39;49mgeneral_prompt(query)\n\u001b[0;32m    313\u001b[0m     answer \u001b[39m=\u001b[39m result[\u001b[39m0\u001b[39m][\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mchoices\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mmessage\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m    314\u001b[0m     function_result \u001b[39m=\u001b[39m result[\u001b[39m1\u001b[39m]\n",
      "File \u001b[1;32mc:\\users\\arik_\\documents\\dokumente\\studium\\master\\llm_seminar\\project\\src\\topicgpt\\TopicPrompting.py:1127\u001b[0m, in \u001b[0;36mTopicPrompting.general_prompt\u001b[1;34m(self, prompt, n_tries)\u001b[0m\n\u001b[0;32m   1125\u001b[0m function_to_call \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunctionNames2Functions[function_name]\n\u001b[0;32m   1126\u001b[0m function_args \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mloads(function_call[\u001b[39m\"\u001b[39m\u001b[39marguments\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m-> 1127\u001b[0m function_response \u001b[39m=\u001b[39m function_to_call(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfunction_args)\n\u001b[0;32m   1128\u001b[0m function_response_json \u001b[39m=\u001b[39m function_response[\u001b[39m0\u001b[39m]\n\u001b[0;32m   1129\u001b[0m function_response_return_output \u001b[39m=\u001b[39m function_response[\u001b[39m1\u001b[39m]\n",
      "File \u001b[1;32mc:\\users\\arik_\\documents\\dokumente\\studium\\master\\llm_seminar\\project\\src\\topicgpt\\TopicPrompting.py:1022\u001b[0m, in \u001b[0;36mTopicPrompting._combine_topics_openai\u001b[1;34m(self, topic_idx_lis, inplace)\u001b[0m\n\u001b[0;32m   1013\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_combine_topics_openai\u001b[39m(\u001b[39mself\u001b[39m, topic_idx_lis: \u001b[39mlist\u001b[39m[\u001b[39mint\u001b[39m], inplace \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m (json, \u001b[39mlist\u001b[39m[Topic]):\n\u001b[0;32m   1014\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1015\u001b[0m \u001b[39m    A version of the combine_topics function that returns a json file to be used with the openai API\u001b[39;00m\n\u001b[0;32m   1016\u001b[0m \u001b[39m    params:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1020\u001b[0m \u001b[39m        json object to be used with the openai API, also returns the new topics\u001b[39;00m\n\u001b[0;32m   1021\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1022\u001b[0m     new_topics \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcombine_topics(topic_idx_lis, inplace)\n\u001b[0;32m   1023\u001b[0m     json_obj \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mdumps({\n\u001b[0;32m   1024\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mnew topics\u001b[39m\u001b[39m\"\u001b[39m: [topic\u001b[39m.\u001b[39mto_dict() \u001b[39mfor\u001b[39;00m topic \u001b[39min\u001b[39;00m new_topics][\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m   1025\u001b[0m     })\n\u001b[0;32m   1026\u001b[0m     \u001b[39mreturn\u001b[39;00m json_obj, new_topics\n",
      "File \u001b[1;32mc:\\users\\arik_\\documents\\dokumente\\studium\\master\\llm_seminar\\project\\src\\topicgpt\\TopicPrompting.py:715\u001b[0m, in \u001b[0;36mTopicPrompting.combine_topics\u001b[1;34m(self, topic_idx_lis, inplace)\u001b[0m\n\u001b[0;32m    711\u001b[0m     new_topic_document_embeddings_hd\u001b[39m.\u001b[39mappend(topic\u001b[39m.\u001b[39mdocument_embeddings_hd)\n\u001b[0;32m    713\u001b[0m new_topic_document_embeddings_hd \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mconcatenate(new_topic_document_embeddings_hd, axis \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m)\n\u001b[1;32m--> 715\u001b[0m new_topic \u001b[39m=\u001b[39m extract_and_describe_topic_cos_sim(\n\u001b[0;32m    716\u001b[0m     documents_topic \u001b[39m=\u001b[39;49m new_topic_docs,\n\u001b[0;32m    717\u001b[0m     document_embeddings_topic \u001b[39m=\u001b[39;49m new_topic_document_embeddings_hd,\n\u001b[0;32m    718\u001b[0m     words_topic \u001b[39m=\u001b[39;49m new_topic_words,\n\u001b[0;32m    719\u001b[0m     vocab_embeddings \u001b[39m=\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvocab_embeddings,\n\u001b[0;32m    720\u001b[0m     umap_mapper \u001b[39m=\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtopic_lis[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49mumap_mapper,\n\u001b[0;32m    721\u001b[0m     enhancer\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menhancer,\n\u001b[0;32m    722\u001b[0m     n_topwords \u001b[39m=\u001b[39;49m \u001b[39m2000\u001b[39;49m\n\u001b[0;32m    723\u001b[0m )\n\u001b[0;32m    725\u001b[0m new_topic\u001b[39m.\u001b[39mtopic_idx \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtopic_lis) \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    726\u001b[0m new_topic_lis \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtopic_lis\u001b[39m.\u001b[39mcopy()\n",
      "File \u001b[1;32mc:\\users\\arik_\\documents\\dokumente\\studium\\master\\llm_seminar\\project\\src\\topicgpt\\TopicRepresentation.py:565\u001b[0m, in \u001b[0;36mextract_and_describe_topic_cos_sim\u001b[1;34m(documents_topic, document_embeddings_topic, words_topic, vocab_embeddings, umap_mapper, enhancer, n_topwords, n_topwords_description)\u001b[0m\n\u001b[0;32m    542\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[0;32m    543\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mextract_and_describe_topic_cos_sim\u001b[39m(documents_topic: \u001b[39mlist\u001b[39m[\u001b[39mstr\u001b[39m], \n\u001b[0;32m    544\u001b[0m                   document_embeddings_topic: np\u001b[39m.\u001b[39mndarray, \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    549\u001b[0m                   n_topwords: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39m2000\u001b[39m, \n\u001b[0;32m    550\u001b[0m                   n_topwords_description \u001b[39m=\u001b[39m \u001b[39m500\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Topic:\n\u001b[0;32m    551\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    552\u001b[0m \u001b[39m    create a topic object from the given documents and embeddings. i.e. compute the centroid and the top-words. Only use cosine-similarity for top-word extraction. Describe and name the topic with the given enhancer object\u001b[39;00m\n\u001b[0;32m    553\u001b[0m \u001b[39m    params:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    563\u001b[0m \u001b[39m        Topic object\u001b[39;00m\n\u001b[0;32m    564\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 565\u001b[0m     topic \u001b[39m=\u001b[39m extract_topic_cos_sim(documents_topic, document_embeddings_topic, words_topic, vocab_embeddings, umap_mapper, n_topwords)\n\u001b[0;32m    566\u001b[0m     topic \u001b[39m=\u001b[39m describe_and_name_topics([topic], enhancer, \u001b[39m\"\u001b[39m\u001b[39mcosine_similarity\u001b[39m\u001b[39m\"\u001b[39m, n_topwords_description)[\u001b[39m0\u001b[39m]\n\u001b[0;32m    567\u001b[0m     \u001b[39mreturn\u001b[39;00m topic\n",
      "File \u001b[1;32mc:\\users\\arik_\\documents\\dokumente\\studium\\master\\llm_seminar\\project\\src\\topicgpt\\TopicRepresentation.py:512\u001b[0m, in \u001b[0;36mextract_topic_cos_sim\u001b[1;34m(documents_topic, document_embeddings_topic, words_topic, vocab_embeddings, umap_mapper, n_topwords)\u001b[0m\n\u001b[0;32m    510\u001b[0m word_topic_mat \u001b[39m=\u001b[39m extractor\u001b[39m.\u001b[39mcompute_word_topic_mat(documents_topic, words_topic, labels, consider_outliers \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m)  \u001b[39m# compute the word-topic matrix of the corpus\u001b[39;00m\n\u001b[0;32m    511\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mcosine_similarity\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m topword_extraction_methods:\n\u001b[1;32m--> 512\u001b[0m     cosine_topwords, cosine_dict \u001b[39m=\u001b[39m extractor\u001b[39m.\u001b[39;49mextract_topwords_centroid_similarity(word_topic_mat \u001b[39m=\u001b[39;49m word_topic_mat, vocab \u001b[39m=\u001b[39;49m words_topic, vocab_embedding_dict \u001b[39m=\u001b[39;49m vocab_embeddings, centroid_dict\u001b[39m=\u001b[39;49m {\u001b[39m0\u001b[39;49m: centroid_ld}, umap_mapper \u001b[39m=\u001b[39;49m umap_mapper, top_n_words \u001b[39m=\u001b[39;49m n_topwords, reduce_vocab_embeddings \u001b[39m=\u001b[39;49m \u001b[39mTrue\u001b[39;49;00m, reduce_centroid_embeddings \u001b[39m=\u001b[39;49m \u001b[39mFalse\u001b[39;49;00m, consider_outliers \u001b[39m=\u001b[39;49m \u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m    516\u001b[0m top_words \u001b[39m=\u001b[39m {\n\u001b[0;32m    517\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcosine_similarity\u001b[39m\u001b[39m\"\u001b[39m: cosine_topwords \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mcosine_similarity\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m topword_extraction_methods \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    518\u001b[0m }\n\u001b[0;32m    519\u001b[0m top_word_scores \u001b[39m=\u001b[39m {\n\u001b[0;32m    520\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcosine_similarity\u001b[39m\u001b[39m\"\u001b[39m: cosine_dict \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mcosine_similarity\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m topword_extraction_methods \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    521\u001b[0m }\n",
      "File \u001b[1;32mc:\\users\\arik_\\documents\\dokumente\\studium\\master\\llm_seminar\\project\\src\\topicgpt\\ExtractTopWords.py:381\u001b[0m, in \u001b[0;36mExtractTopWords.extract_topwords_centroid_similarity\u001b[1;34m(self, word_topic_mat, vocab, vocab_embedding_dict, centroid_dict, umap_mapper, top_n_words, reduce_vocab_embeddings, reduce_centroid_embeddings, consider_outliers)\u001b[0m\n\u001b[0;32m    363\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mextract_topwords_centroid_similarity\u001b[39m(\u001b[39mself\u001b[39m, word_topic_mat: np\u001b[39m.\u001b[39mndarray, vocab: \u001b[39mlist\u001b[39m[\u001b[39mstr\u001b[39m], vocab_embedding_dict: \u001b[39mdict\u001b[39m, centroid_dict: \u001b[39mdict\u001b[39m, umap_mapper: umap\u001b[39m.\u001b[39mUMAP, top_n_words: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m, reduce_vocab_embeddings: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m, reduce_centroid_embeddings: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m, consider_outliers: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m (\u001b[39mdict\u001b[39m, np\u001b[39m.\u001b[39mndarray):\n\u001b[0;32m    364\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    365\u001b[0m \u001b[39m    Extract the top-words for each cluster by computing the cosine similarity of the words that occur in the corpus to the centroid of the cluster\u001b[39;00m\n\u001b[0;32m    366\u001b[0m \u001b[39m    params: \u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    379\u001b[0m \u001b[39m        np.ndarray, cosine similarity of each word in the vocab to each centroid. has shape (len(vocab), len(centroid_dict) - 1)\u001b[39;00m\n\u001b[0;32m    380\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 381\u001b[0m     similarity_mat \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_embedding_similarity_centroids(vocab, vocab_embedding_dict, umap_mapper, centroid_dict, reduce_vocab_embeddings, reduce_centroid_embeddings)\n\u001b[0;32m    382\u001b[0m     top_words \u001b[39m=\u001b[39m {}\n\u001b[0;32m    383\u001b[0m     top_word_scores \u001b[39m=\u001b[39m {}\n",
      "File \u001b[1;32mc:\\users\\arik_\\documents\\dokumente\\studium\\master\\llm_seminar\\project\\src\\topicgpt\\ExtractTopWords.py:356\u001b[0m, in \u001b[0;36mExtractTopWords.compute_embedding_similarity_centroids\u001b[1;34m(self, vocab, vocab_embedding_dict, umap_mapper, centroid_dict, reduce_vocab_embeddings, reduce_centroid_embeddings)\u001b[0m\n\u001b[0;32m    354\u001b[0m     vocab_arr[i] \u001b[39m=\u001b[39m vocab_embedding_dict[word]\n\u001b[0;32m    355\u001b[0m \u001b[39mif\u001b[39;00m reduce_vocab_embeddings:\n\u001b[1;32m--> 356\u001b[0m     vocab_arr \u001b[39m=\u001b[39m umap_mapper\u001b[39m.\u001b[39;49mtransform(vocab_arr)\n\u001b[0;32m    358\u001b[0m vocab_arr \u001b[39m=\u001b[39m vocab_arr \u001b[39m/\u001b[39m np\u001b[39m.\u001b[39mlinalg\u001b[39m.\u001b[39mnorm(vocab_arr, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,\u001b[39m1\u001b[39m)\n\u001b[0;32m    360\u001b[0m similarity \u001b[39m=\u001b[39m vocab_arr \u001b[39m@\u001b[39m centroid_arr\u001b[39m.\u001b[39mT \u001b[39m# cosine similarity\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\arik_\\anaconda3\\envs\\llm_sem\\Lib\\site-packages\\umap\\umap_.py:2896\u001b[0m, in \u001b[0;36mUMAP.transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m   2894\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   2895\u001b[0m     epsilon \u001b[39m=\u001b[39m \u001b[39m0.24\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_knn_search_index\u001b[39m.\u001b[39m_angular_trees \u001b[39melse\u001b[39;00m \u001b[39m0.12\u001b[39m\n\u001b[1;32m-> 2896\u001b[0m     indices, dists \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_knn_search_index\u001b[39m.\u001b[39;49mquery(\n\u001b[0;32m   2897\u001b[0m         X, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_neighbors, epsilon\u001b[39m=\u001b[39;49mepsilon\n\u001b[0;32m   2898\u001b[0m     )\n\u001b[0;32m   2900\u001b[0m dists \u001b[39m=\u001b[39m dists\u001b[39m.\u001b[39mastype(np\u001b[39m.\u001b[39mfloat32, order\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mC\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   2901\u001b[0m \u001b[39m# Remove any nearest neighbours who's distances are greater than our disconnection_distance\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\arik_\\anaconda3\\envs\\llm_sem\\Lib\\site-packages\\pynndescent\\pynndescent_.py:1686\u001b[0m, in \u001b[0;36mNNDescent.query\u001b[1;34m(self, query_data, k, epsilon)\u001b[0m\n\u001b[0;32m   1683\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_search_function()\n\u001b[0;32m   1685\u001b[0m     query_data \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(query_data)\u001b[39m.\u001b[39mastype(np\u001b[39m.\u001b[39mfloat32, order\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mC\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> 1686\u001b[0m     indices, dists, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_search_function(\n\u001b[0;32m   1687\u001b[0m         query_data, k, epsilon, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_visited, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msearch_rng_state\n\u001b[0;32m   1688\u001b[0m     )\n\u001b[0;32m   1689\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1690\u001b[0m     \u001b[39m# Sparse case\u001b[39;00m\n\u001b[0;32m   1691\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m_search_function\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "\u001b[1;31mSystemError\u001b[0m: CPUDispatcher(<function NNDescent._init_search_function.<locals>.search_closure at 0x0000026C0365B600>) returned a result with an exception set"
     ]
    }
   ],
   "source": [
    "tm.pprompt(\"please merge topic 0 and topic 2. Do this inplace\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are interested in very negative reviews, we want to create a new topic based on this idea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT wants to the call the function:  {\n",
      "  \"name\": \"add_new_topic_keyword\",\n",
      "  \"arguments\": \"{\\n  \\\"keyword\\\": \\\"very negative sentiment\\\"\\n}\"\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs completed: 100%| ██████████ 100/100 [00:02]\n",
      "Epochs completed: 100%| ██████████ 100/100 [00:01]\n",
      "Computing word-topic matrix: 100%|██████████| 52/52 [00:08<00:00,  6.31it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'dim_red_centroid_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m tm\u001b[39m.\u001b[39;49mpprompt(\u001b[39m\"\u001b[39;49m\u001b[39mPlease create a new topic that encompasses documents with a very negative sentiment.\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\users\\arik_\\documents\\dokumente\\studium\\master\\llm_seminar\\project\\src\\topicgpt\\TopicGPT.py:330\u001b[0m, in \u001b[0;36mTopicGPT.pprompt\u001b[1;34m(self, query, return_function_result)\u001b[0m\n\u001b[0;32m    320\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpprompt\u001b[39m(\u001b[39mself\u001b[39m, query:\u001b[39mstr\u001b[39m, return_function_result: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mobject\u001b[39m:\n\u001b[0;32m    321\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    322\u001b[0m \u001b[39m    This function prompts the model with the query and prints the answer.\u001b[39;00m\n\u001b[0;32m    323\u001b[0m \u001b[39m    params:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[39m        function_result: The result of the function call.    \u001b[39;00m\n\u001b[0;32m    328\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 330\u001b[0m     answer, function_result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprompt(query)\n\u001b[0;32m    332\u001b[0m     \u001b[39mprint\u001b[39m(answer)\n\u001b[0;32m    334\u001b[0m     \u001b[39mif\u001b[39;00m return_function_result:\n",
      "File \u001b[1;32mc:\\users\\arik_\\documents\\dokumente\\studium\\master\\llm_seminar\\project\\src\\topicgpt\\TopicGPT.py:311\u001b[0m, in \u001b[0;36mTopicGPT.prompt\u001b[1;34m(self, query)\u001b[0m\n\u001b[0;32m    301\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprompt\u001b[39m(\u001b[39mself\u001b[39m, query: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m (\u001b[39mstr\u001b[39m, \u001b[39mobject\u001b[39m):\n\u001b[0;32m    302\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    303\u001b[0m \u001b[39m    This function prompts the model with the query. Please Have a look at the TopicPrompting class for more details on available functions for prompting the model.\u001b[39;00m\n\u001b[0;32m    304\u001b[0m \u001b[39m    params:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    308\u001b[0m \u001b[39m        function_result: The result of the function call.\u001b[39;00m\n\u001b[0;32m    309\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 311\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtopic_prompting\u001b[39m.\u001b[39;49mgeneral_prompt(query)\n\u001b[0;32m    313\u001b[0m     answer \u001b[39m=\u001b[39m result[\u001b[39m0\u001b[39m][\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mchoices\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mmessage\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m    314\u001b[0m     function_result \u001b[39m=\u001b[39m result[\u001b[39m1\u001b[39m]\n",
      "File \u001b[1;32mc:\\users\\arik_\\documents\\dokumente\\studium\\master\\llm_seminar\\project\\src\\topicgpt\\TopicPrompting.py:1127\u001b[0m, in \u001b[0;36mTopicPrompting.general_prompt\u001b[1;34m(self, prompt, n_tries)\u001b[0m\n\u001b[0;32m   1125\u001b[0m function_to_call \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunctionNames2Functions[function_name]\n\u001b[0;32m   1126\u001b[0m function_args \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mloads(function_call[\u001b[39m\"\u001b[39m\u001b[39marguments\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m-> 1127\u001b[0m function_response \u001b[39m=\u001b[39m function_to_call(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfunction_args)\n\u001b[0;32m   1128\u001b[0m function_response_json \u001b[39m=\u001b[39m function_response[\u001b[39m0\u001b[39m]\n\u001b[0;32m   1129\u001b[0m function_response_return_output \u001b[39m=\u001b[39m function_response[\u001b[39m1\u001b[39m]\n",
      "File \u001b[1;32mc:\\users\\arik_\\documents\\dokumente\\studium\\master\\llm_seminar\\project\\src\\topicgpt\\TopicPrompting.py:1043\u001b[0m, in \u001b[0;36mTopicPrompting._add_new_topic_keyword_openai\u001b[1;34m(self, keyword, inplace, rename_new_topic)\u001b[0m\n\u001b[0;32m   1028\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_add_new_topic_keyword_openai\u001b[39m(\u001b[39mself\u001b[39m, keyword: \u001b[39mstr\u001b[39m, inplace:\u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m, rename_new_topic:\u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m (json, \u001b[39mlist\u001b[39m[Topic]):\n\u001b[0;32m   1029\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1030\u001b[0m \u001b[39m    A version of the add_new_topic_keyword function that returns a json file to be used with the openai API\u001b[39;00m\n\u001b[0;32m   1031\u001b[0m \u001b[39m    params:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1041\u001b[0m \u001b[39m        json object to be used with the openai API\u001b[39;00m\n\u001b[0;32m   1042\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1043\u001b[0m     new_topics \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madd_new_topic_keyword(keyword, inplace, rename_new_topic)\n\u001b[0;32m   1044\u001b[0m     json_obj \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mdumps({\n\u001b[0;32m   1045\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mnew topics\u001b[39m\u001b[39m\"\u001b[39m: [topic\u001b[39m.\u001b[39mto_dict() \u001b[39mfor\u001b[39;00m topic \u001b[39min\u001b[39;00m new_topics][\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m   1046\u001b[0m     })\n\u001b[0;32m   1047\u001b[0m     \u001b[39mreturn\u001b[39;00m json_obj, new_topics\n",
      "File \u001b[1;32mc:\\users\\arik_\\documents\\dokumente\\studium\\master\\llm_seminar\\project\\src\\topicgpt\\TopicPrompting.py:798\u001b[0m, in \u001b[0;36mTopicPrompting.add_new_topic_keyword\u001b[1;34m(self, keyword, inplace, rename_new_topic)\u001b[0m\n\u001b[0;32m    795\u001b[0m new_embeddings_hd \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mconcatenate(new_embeddings_hd, axis \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m)\n\u001b[0;32m    796\u001b[0m new_embeddings_ld \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mconcatenate(new_embeddings_ld, axis \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m)\n\u001b[1;32m--> 798\u001b[0m new_topics \u001b[39m=\u001b[39m extract_describe_topics_labels_vocab(\n\u001b[0;32m    799\u001b[0m     corpus \u001b[39m=\u001b[39;49m doc_lis,\n\u001b[0;32m    800\u001b[0m     document_embeddings_hd \u001b[39m=\u001b[39;49m new_embeddings_hd,\n\u001b[0;32m    801\u001b[0m     document_embeddings_ld \u001b[39m=\u001b[39;49m new_embeddings_ld,\n\u001b[0;32m    802\u001b[0m     labels \u001b[39m=\u001b[39;49m new_doc_topic_assignments,\n\u001b[0;32m    803\u001b[0m     vocab \u001b[39m=\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvocab,\n\u001b[0;32m    804\u001b[0m     umap_mapper \u001b[39m=\u001b[39;49m umap_mapper,\n\u001b[0;32m    805\u001b[0m     vocab_embeddings \u001b[39m=\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvocab_embeddings, \n\u001b[0;32m    806\u001b[0m     enhancer \u001b[39m=\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menhancer\n\u001b[0;32m    807\u001b[0m )\n\u001b[0;32m    809\u001b[0m \u001b[39mif\u001b[39;00m rename_new_topic:\n\u001b[0;32m    810\u001b[0m     new_topics[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mtopic_name \u001b[39m=\u001b[39m keyword\n",
      "File \u001b[1;32mc:\\users\\arik_\\documents\\dokumente\\studium\\master\\llm_seminar\\project\\src\\topicgpt\\TopicRepresentation.py:479\u001b[0m, in \u001b[0;36mextract_describe_topics_labels_vocab\u001b[1;34m(corpus, document_embeddings_hd, document_embeddings_ld, labels, umap_mapper, vocab_embeddings, enhancer, vocab, n_topwords, n_topwords_description, topword_extraction_methods, topword_description_method)\u001b[0m\n\u001b[0;32m    456\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[0;32m    457\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mextract_describe_topics_labels_vocab\u001b[39m(corpus: \u001b[39mlist\u001b[39m[\u001b[39mstr\u001b[39m], document_embeddings_hd: np\u001b[39m.\u001b[39mndarray, document_embeddings_ld: np\u001b[39m.\u001b[39mndarray, labels: np\u001b[39m.\u001b[39mndarray, umap_mapper: umap\u001b[39m.\u001b[39mUMAP, vocab_embeddings: np\u001b[39m.\u001b[39mndarray, enhancer: TopwordEnhancement, vocab: \u001b[39mlist\u001b[39m[\u001b[39mstr\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, n_topwords: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39m2000\u001b[39m, n_topwords_description \u001b[39m=\u001b[39m \u001b[39m500\u001b[39m, topword_extraction_methods: \u001b[39mlist\u001b[39m[\u001b[39mstr\u001b[39m] \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mtfidf\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcosine_similarity\u001b[39m\u001b[39m\"\u001b[39m], topword_description_method \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcosine_similarity\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mlist\u001b[39m[Topic]:\n\u001b[0;32m    458\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    459\u001b[0m \u001b[39m    Extract the topics from the given corpus by using the provided labels that indicate the topics. (No -1 for outliers!). Also the vocab is already computed. Describe and name the topics with the given enhancer object.\u001b[39;00m\n\u001b[0;32m    460\u001b[0m \u001b[39m    Otherwise same as extract_topics\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    477\u001b[0m \n\u001b[0;32m    478\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 479\u001b[0m     topics \u001b[39m=\u001b[39m extract_topics_labels_vocab(corpus, document_embeddings_hd, document_embeddings_ld, labels, umap_mapper, vocab_embeddings, vocab, n_topwords, topword_extraction_methods)\n\u001b[0;32m    480\u001b[0m     topics \u001b[39m=\u001b[39m describe_and_name_topics(topics, enhancer, topword_description_method, n_topwords_description)\n\u001b[0;32m    481\u001b[0m     \u001b[39mreturn\u001b[39;00m topics\n",
      "File \u001b[1;32mc:\\users\\arik_\\documents\\dokumente\\studium\\master\\llm_seminar\\project\\src\\topicgpt\\TopicRepresentation.py:409\u001b[0m, in \u001b[0;36mextract_topics_labels_vocab\u001b[1;34m(corpus, document_embeddings_hd, document_embeddings_ld, labels, umap_mapper, vocab_embeddings, vocab, n_topwords, topword_extraction_methods)\u001b[0m\n\u001b[0;32m    407\u001b[0m     tfidf_topwords, tfidf_dict \u001b[39m=\u001b[39m extractor\u001b[39m.\u001b[39mextract_topwords_tfidf(word_topic_mat \u001b[39m=\u001b[39m word_topic_mat, vocab \u001b[39m=\u001b[39m vocab, labels \u001b[39m=\u001b[39m labels, top_n_words \u001b[39m=\u001b[39m n_topwords)  \u001b[39m# extract the top-words according to tfidf\u001b[39;00m\n\u001b[0;32m    408\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mcosine_similarity\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m topword_extraction_methods:\n\u001b[1;32m--> 409\u001b[0m     cosine_topwords, cosine_dict \u001b[39m=\u001b[39m extractor\u001b[39m.\u001b[39mextract_topwords_centroid_similarity(word_topic_mat \u001b[39m=\u001b[39m word_topic_mat, vocab \u001b[39m=\u001b[39m vocab, vocab_embedding_dict \u001b[39m=\u001b[39m vocab_embeddings, centroid_dict\u001b[39m=\u001b[39m dim_red_centroid_dict, umap_mapper \u001b[39m=\u001b[39m umap_mapper, top_n_words \u001b[39m=\u001b[39m n_topwords, reduce_vocab_embeddings \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m, reduce_centroid_embeddings \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m, consider_outliers \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    411\u001b[0m topics \u001b[39m=\u001b[39m []\n\u001b[0;32m    412\u001b[0m \u001b[39mfor\u001b[39;00m i, label \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(np\u001b[39m.\u001b[39munique(labels)):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dim_red_centroid_dict' is not defined"
     ]
    }
   ],
   "source": [
    "tm.pprompt(\"Please create a new topic that encompasses documents with a very negative sentiment.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "On the other hand, topics 68 and 69 seem a bit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT wants to the call the function:  {\n",
      "  \"name\": \"combine_topics\",\n",
      "  \"arguments\": \"{\\n  \\\"topic_idx_lis\\\": [68, 69],\\n  \\\"inplace\\\": true\\n}\"\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs completed: 100%| ██████████ 100/100 [00:01]\n",
      "Computing word-topic matrix: 100%|██████████| 1/1 [00:00<00:00, 10.32it/s]\n",
      "Epochs completed: 100%| ██████████ 30/30 [00:02]\n",
      "Epochs completed: 100%| ██████████ 100/100 [00:01]\n",
      "100%|██████████| 1/1 [00:06<00:00,  6.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics 68 and 69 have been combined into a new topic, identified as Topic 73: \"Fantasy Literature\". This topic encompasses various aspects and sub-topics, including creatures like dwarves, elves, trolls, goblins, and wizards, as well as magical elements such as spells, enchanting, fables, and magical creatures. Additionally, the topic includes elements of adventure and exploration, such as tracking, thrills, and exploring, as well as characters and storytelling, including imaginative characters, castles, stories, and narrators. Furthermore, the topic may also involve critiques and opinions related to imaginative literature, including words like unimaginative, overused, disliked, and unoriginal. Please note that these sub-topics are inferred based on the provided words and may not represent the complete range of topics in the corpus. \n",
      "\n",
      "used function call: {\n",
      "  \"name\": \"combine_topics\",\n",
      "  \"arguments\": \"{\\n  \\\"topic_idx_lis\\\": [68, 69],\\n  \\\"inplace\\\": true\\n}\"\n",
      "} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "tm.pprompt(\"please combine topics 68 and 69. Do this inplace\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Topic 0: Music genres,\n",
       " Topic 1: Diapers,\n",
       " Topic 2: Superhero Movies,\n",
       " Topic 3: Movies and Filmmaking,\n",
       " Topic 4: Pest Control,\n",
       " Topic 5: Dystopian Society,\n",
       " Topic 6: Cooking,\n",
       " Topic 7: Classic Literature,\n",
       " Topic 8: Survival and Savagery.,\n",
       " Topic 9: Video Games,\n",
       " Topic 10: Printer-related issues,\n",
       " Topic 11: Camera Accessories,\n",
       " Topic 12: Massage Therapy,\n",
       " Topic 13: Adventure and Mystery.,\n",
       " Topic 14: Product Reviews,\n",
       " Topic 15: Health supplements,\n",
       " Topic 16: Pet Treats,\n",
       " Topic 17: Whaling and Survival.,\n",
       " Topic 18: Inflatable Airbeds,\n",
       " Topic 19: Product Defects,\n",
       " Topic 20: Prehistoric hunting,\n",
       " Topic 21: Book Reviews,\n",
       " Topic 22: Coffee and Beverage,\n",
       " Topic 23: Coffee Brewing Equipment,\n",
       " Topic 24: Product reviews,\n",
       " Topic 25: Plumbing Accessories,\n",
       " Topic 26: Appliances,\n",
       " Topic 27: Kitchen Appliances,\n",
       " Topic 28: Kitchen Utensils,\n",
       " Topic 29: Relationships and Dating,\n",
       " Topic 30: Bicycle Accessories,\n",
       " Topic 31: Product Features,\n",
       " Topic 32: Children's Toys,\n",
       " Topic 33: baby products,\n",
       " Topic 34: Product Recommendation,\n",
       " Topic 35: Hardware Tools,\n",
       " Topic 36: Electronics connectivity,\n",
       " Topic 37: Networking and Connectivity,\n",
       " Topic 38: Flashlights,\n",
       " Topic 39: Audio/Video Connectivity,\n",
       " Topic 40: Shiny Objects,\n",
       " Topic 41: Shoes,\n",
       " Topic 42: Clothing quality and fit.,\n",
       " Topic 43: Gardening,\n",
       " Topic 44: Product Reviews,\n",
       " Topic 45: Electronic Accessories,\n",
       " Topic 46: Electronic Devices,\n",
       " Topic 47: Language Learning,\n",
       " Topic 48: Audio Equipment,\n",
       " Topic 49: Electronics,\n",
       " Topic 50: Online Shopping Experience,\n",
       " Topic 51: Defective electronic devices.,\n",
       " Topic 52: Book Reviews,\n",
       " Topic 53: Book Formatting,\n",
       " Topic 54: Investing in Stocks,\n",
       " Topic 55: Book genres,\n",
       " Topic 56: Programming Books,\n",
       " Topic 57: Book Reviews,\n",
       " Topic 58: Art and Design,\n",
       " Topic 59: Legal Proceedings,\n",
       " Topic 60: book genres,\n",
       " Topic 61: Book Criticisms,\n",
       " Topic 62: Dystopian literature,\n",
       " Topic 63: Religious Controversies,\n",
       " Topic 64: Self-help and spirituality.,\n",
       " Topic 65: Christianity and Prayer,\n",
       " Topic 66: book genres,\n",
       " Topic 67: Book genres,\n",
       " Topic 68: Fantasy Books,\n",
       " Topic 69: Nature and Adventure,\n",
       " Topic 70: Historical Events,\n",
       " Topic 71: Family and Childhood,\n",
       " Topic 72: Books and Reading,\n",
       " Topic 73: Fantasy Literature]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tm.topic_lis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_sem",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
